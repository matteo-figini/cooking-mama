{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5245331,"sourceType":"datasetVersion","datasetId":3052101}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":59.732213,"end_time":"2025-04-27T07:29:44.700379","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-04-27T07:28:44.968166","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Language Processing Course Project\n### Politecnico di Milano, A.Y. 2024/2025\n\nThis notebook has been created for the Natural Language Processing course, held by Prof. Carman.\n\n**Authors**:\n* Matteo Figini\n* Riccardo Figini\n* Samuele Forner\n* Caterina Motti\n* Simone Zacchetti","metadata":{"papermill":{"duration":0.002713,"end_time":"2025-04-27T07:28:49.620828","exception":false,"start_time":"2025-04-27T07:28:49.618115","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"The purpose of this notebook is to apply different NLP techniques to analyse the **RecipeNLG** dataset, you can find the dataset at the following [link](https://huggingface.co/datasets/mbien/recipe_nlg).","metadata":{"papermill":{"duration":0.001809,"end_time":"2025-04-27T07:28:49.625064","exception":false,"start_time":"2025-04-27T07:28:49.623255","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Import useful libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport ast\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport random","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-04-30T09:35:59.064090Z","iopub.execute_input":"2025-04-30T09:35:59.064449Z","iopub.status.idle":"2025-04-30T09:35:59.069663Z","shell.execute_reply.started":"2025-04-30T09:35:59.064423Z","shell.execute_reply":"2025-04-30T09:35:59.068475Z"},"papermill":{"duration":1.901977,"end_time":"2025-04-27T07:28:51.529021","exception":false,"start_time":"2025-04-27T07:28:49.627044","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preliminary analysis","metadata":{"papermill":{"duration":0.001915,"end_time":"2025-04-27T07:28:51.533282","exception":false,"start_time":"2025-04-27T07:28:51.531367","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"In this section, we aim to perform a preliminary analysis of the dataset.\n- Load and inspect the dataset: analyze which field are present, missing field or inconsistencies. \n- Analyze recipe distribution: compute statistics and visualize them.\n- Analyze entities distribution: compute statistics and visualize them.","metadata":{}},{"cell_type":"markdown","source":"### Load and inspect the dataset","metadata":{}},{"cell_type":"code","source":"# Load the dataset from CSV file and show its head\ndf = pd.read_csv(\"/kaggle/input/recipenlg/dataset/full_dataset.csv\")\ndf","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:11:54.281042Z","iopub.execute_input":"2025-04-30T09:11:54.281564Z","iopub.status.idle":"2025-04-30T09:12:47.455957Z","shell.execute_reply.started":"2025-04-30T09:11:54.281531Z","shell.execute_reply":"2025-04-30T09:12:47.455142Z"},"papermill":{"duration":51.344256,"end_time":"2025-04-27T07:29:42.879584","exception":false,"start_time":"2025-04-27T07:28:51.535328","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show the name of the columns\nprint(\"Columns:\", df.columns.tolist())","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:12:47.456761Z","iopub.execute_input":"2025-04-30T09:12:47.457433Z","iopub.status.idle":"2025-04-30T09:12:47.463122Z","shell.execute_reply.started":"2025-04-30T09:12:47.457373Z","shell.execute_reply":"2025-04-30T09:12:47.461948Z"},"papermill":{"duration":0.009406,"end_time":"2025-04-27T07:29:42.891303","exception":false,"start_time":"2025-04-27T07:29:42.881897","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since the first column has no name, we renamed it:","metadata":{}},{"cell_type":"code","source":"df = df.rename(columns={'Unnamed: 0': 'id'})","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:12:47.464128Z","iopub.execute_input":"2025-04-30T09:12:47.464483Z","iopub.status.idle":"2025-04-30T09:12:47.731462Z","shell.execute_reply.started":"2025-04-30T09:12:47.464451Z","shell.execute_reply":"2025-04-30T09:12:47.730670Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset length\nprint(f\"In the dataset there are {len(df)} different recipes!\")","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:12:47.733491Z","iopub.execute_input":"2025-04-30T09:12:47.733766Z","iopub.status.idle":"2025-04-30T09:12:47.738764Z","shell.execute_reply.started":"2025-04-30T09:12:47.733744Z","shell.execute_reply":"2025-04-30T09:12:47.737755Z"},"papermill":{"duration":0.009012,"end_time":"2025-04-27T07:29:43.778869","exception":false,"start_time":"2025-04-27T07:29:43.769857","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we can see, the dataset contains 2231142 entries in 7 columns:\n- id (int): ID.\n- title (str): title of the recipe.\n- ingredients (list of str): ingredients.\n- directions (list of str): instruction steps.\n- link (str): URL link.\n- source (ClassLabel): origin of each recipe record, with possible value {\"Gathered\", \"Recipes1M\"}:\n    - \"Gathered\" (0): additional recipes gathered from multiple cooking web pages using automated scripts in a web scraping process.\n    - \"Recipes1M\" (1): recipes from the \"Recipe1M+\" dataset.\n- NER (list of str): NER food entities.","metadata":{}},{"cell_type":"markdown","source":"Now, we check for **inconsistencies** and remove them if needed.","metadata":{}},{"cell_type":"code","source":"# Rows with missing values\nrows_with_null = df.isnull().any(axis=1).sum()\nprint(f\"There are {rows_with_null} rows with at least one null value.\")\nprint(df.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:12:47.739609Z","iopub.execute_input":"2025-04-30T09:12:47.739890Z","iopub.status.idle":"2025-04-30T09:12:49.842376Z","shell.execute_reply.started":"2025-04-30T09:12:47.739870Z","shell.execute_reply":"2025-04-30T09:12:49.841432Z"},"papermill":{"duration":0.872436,"end_time":"2025-04-27T07:29:43.767295","exception":false,"start_time":"2025-04-27T07:29:42.894859","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The missing value is a title, which is not a critical field. However, we prefer to remove it.","metadata":{}},{"cell_type":"code","source":"# Delete rows with at least one missing value\ndf.dropna()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:12:49.843364Z","iopub.execute_input":"2025-04-30T09:12:49.843714Z","iopub.status.idle":"2025-04-30T09:12:51.218180Z","shell.execute_reply.started":"2025-04-30T09:12:49.843683Z","shell.execute_reply":"2025-04-30T09:12:51.217211Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find duplicates by title\nduplicate_counts = df['title'].value_counts()\nprint(\"Duplication results by: \", duplicate_counts)","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:12:51.219091Z","iopub.execute_input":"2025-04-30T09:12:51.219388Z","iopub.status.idle":"2025-04-30T09:12:52.808205Z","shell.execute_reply.started":"2025-04-30T09:12:51.219357Z","shell.execute_reply":"2025-04-30T09:12:52.806965Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we have lots of duplicates. However, they might represent different variations of the same recipe which could be useful for further analysis.\n\nSince the dataset is really big (2M+ entries), we decided to make a random subsampling of it by retaining only 25% (around half million entries).","metadata":{}},{"cell_type":"code","source":"# Randomly sample the dataset\ndf = df.sample(frac=0.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:12:52.809146Z","iopub.execute_input":"2025-04-30T09:12:52.809427Z","iopub.status.idle":"2025-04-30T09:12:53.901246Z","shell.execute_reply.started":"2025-04-30T09:12:52.809380Z","shell.execute_reply":"2025-04-30T09:12:53.900319Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Analyze vocaboulary","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nfrom nltk import bigrams\n\n# Download necessary NLTK data\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:12:53.902158Z","iopub.execute_input":"2025-04-30T09:12:53.902472Z","iopub.status.idle":"2025-04-30T09:12:55.566636Z","shell.execute_reply.started":"2025-04-30T09:12:53.902446Z","shell.execute_reply":"2025-04-30T09:12:55.565762Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a new column 'text' with directions as a string\ndf['text'] = df['directions'].apply(lambda x: ' '.join(ast.literal_eval(x)))\nall_instructions_list = df['text'].tolist()\n\n# Preprocess the text and tokenize it into words\nfull_text = ' '.join(all_instructions_list)\nfull_text = full_text.lower()\nfull_text = re.sub(r'[^a-zA-Z0-9\\s]', '', full_text)\nwords = full_text.split()\n\n# Remove stopwords and tokenize\nstop_words = set(stopwords.words('english'))\nwords_nostopwords = [w for w in words if w not in stop_words]\ntokens = [w for w in words_nostopwords]\n\n# Generate bigrams and frequency distribution\nbigram_list = list(bigrams(tokens))\nbigram_fdist = FreqDist(bigram_list)\n\n# Frequency distribution of words\nfdist = FreqDist(words_nostopwords)","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:12:55.567498Z","iopub.execute_input":"2025-04-30T09:12:55.568463Z","iopub.status.idle":"2025-04-30T09:14:49.713371Z","shell.execute_reply.started":"2025-04-30T09:12:55.568440Z","shell.execute_reply":"2025-04-30T09:14:49.711922Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Vocabulary size\nvocabulary_size = len(fdist)\nprint(\"Vocabulary size: \", vocabulary_size)\n\n# Average number of unique words per recipe\nunique_words_per_recipe = []\n\nfor directions in df['text']:\n    directions = directions.lower()\n    directions = re.sub(r'[^a-zA-Z0-9\\s]', '', directions)  # remove punctuation\n    tokens = directions.split()  # split into words\n    tokens = [w for w in tokens if w not in stop_words]\n    unique_words = set(tokens)\n    unique_words_per_recipe.append(len(unique_words))\n\navg_unique_words_per_recipe = sum(unique_words_per_recipe) / len(unique_words_per_recipe)    \nprint(\"Average number of unique words per recipe: \", avg_unique_words_per_recipe)\n\n# Compute word length distribution\nword_lengths = [len(word) for word in words_nostopwords]\navg_word_length = sum(word_lengths) / len(word_lengths)\nmin_word_length = min(word_lengths)\nmax_word_length = max(word_lengths)\n\nprint(\"Average word length: \", avg_word_length, \" characters\")\nprint(\"Minimum word length: \", min_word_length, \" characters\")\nprint(\"Maximum word length: \", max_word_length, \" characters\")","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:14:49.714805Z","iopub.execute_input":"2025-04-30T09:14:49.715127Z","iopub.status.idle":"2025-04-30T09:15:08.453542Z","shell.execute_reply.started":"2025-04-30T09:14:49.715092Z","shell.execute_reply":"2025-04-30T09:15:08.452290Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The high vocabulary size shows that the dataset is very rich and diverse, covering many ingredients, actions, and contextual descriptions. \n\nHowever the average number of unique words in each recipe is quite small: this suggests that each recipe tends to use a fairly concise and domain-specific subset.\n\nNB: the maximum word length (262 characters) is likely an outlier. We must investigate it further. ","metadata":{}},{"cell_type":"code","source":"# Find the longest words\ndef find_long_words_in_directions(df):\n    long_words = []\n    \n    for directions in df['text']:\n        # Extract words longer than 45 characters, since in English vocabulary the longest word has 45 characters\n        long_words.extend([word for word in directions.split() if len(word) > 45])\n    \n    if long_words:\n        print(\"Found \", len(long_words), \" long words in the dataset (greater than 45 characters).\")\n        print(\"Printing the first 10 for brevity: \\n\")\n        print(long_words[:10])\n    else:\n        print(\"No words longer than 45 characters found.\")\n\nfind_long_words_in_directions(df)","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:15:08.454608Z","iopub.execute_input":"2025-04-30T09:15:08.454872Z","iopub.status.idle":"2025-04-30T09:15:13.156733Z","shell.execute_reply.started":"2025-04-30T09:15:08.454851Z","shell.execute_reply":"2025-04-30T09:15:13.155635Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Most of them are links, so we can keep them (also considering that they are not that frequent).","metadata":{}},{"cell_type":"code","source":"# Create frequency distribution for bigrams\nbigram_fdist = FreqDist(bigram_list)\n\n# Display top 10 most common words\nprint(\"\\nTop 10 most common words:\")\nfor word, freq in fdist.most_common(10):\n    print(f\"{word}: {freq}\")\n\n# Display top 10 most common bigrams\nprint(\"\\nTop 10 most common bigrams:\")\nfor bigram, freq in bigram_fdist.most_common(10):\n    print(f\"{' '.join(bigram)}: {freq}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:15:13.161598Z","iopub.execute_input":"2025-04-30T09:15:13.161881Z","iopub.status.idle":"2025-04-30T09:15:52.460526Z","shell.execute_reply.started":"2025-04-30T09:15:13.161862Z","shell.execute_reply":"2025-04-30T09:15:52.459429Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Analyze recipes distribution\nSince the recipes are the core of the dataset, it may be useful to analyse the length of each recipe to plot their distribution.\n\nRecipes with very high or very low word counts might be outliers. Analyzing the distribution allows us to detect these outliers, which might skew our analysis. For instance, a recipe with only one word might be a mistake.","metadata":{}},{"cell_type":"code","source":"# Define a new column to count the number of words in 'directions'\ndf['dir_word_count'] = df['directions'].apply(lambda x: len(' '.join(x).split()))","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:15:52.461493Z","iopub.execute_input":"2025-04-30T09:15:52.461755Z","iopub.status.idle":"2025-04-30T09:16:03.406533Z","shell.execute_reply.started":"2025-04-30T09:15:52.461735Z","shell.execute_reply":"2025-04-30T09:16:03.405498Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Retrieve the length and index of the longest and shortest recipe\nmax_words = df['dir_word_count'].max()\nmin_words = df['dir_word_count'].min()\n\nmax_words_idx = df['dir_word_count'].idxmax()\nmin_words_idx = df['dir_word_count'].idxmin()\nprint(f\"Recipes' direction have between {min_words} and {max_words} words.\")\nprint(f\"The longest recipe has {max_words} words and it stands at location {max_words_idx}.\")\n#print(df['directions'].loc[max_words_idx])\nprint(df[['title','directions', 'dir_word_count']].loc[min_words_idx])","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:16:03.407388Z","iopub.execute_input":"2025-04-30T09:16:03.407715Z","iopub.status.idle":"2025-04-30T09:16:03.557825Z","shell.execute_reply.started":"2025-04-30T09:16:03.407693Z","shell.execute_reply":"2025-04-30T09:16:03.556985Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize the distribution of the number of words in column \"directions\" to show how long recipes are (in number of words)\nplt.figure(figsize=(12,6))\nplt.hist(df['dir_word_count'], bins=max_words, range=(0, max_words))\nplt.title('Distribution of words in column \\\"directions\\\"')\nplt.xlabel('Number of words in the recipe')\nplt.ylabel('Number of recipes')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:16:03.558924Z","iopub.execute_input":"2025-04-30T09:16:03.559261Z","iopub.status.idle":"2025-04-30T09:16:30.810837Z","shell.execute_reply.started":"2025-04-30T09:16:03.559222Z","shell.execute_reply":"2025-04-30T09:16:30.809895Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Analyzing recipe word count distribution\nword_counts = df['dir_word_count'].dropna()\nmean_word_count = word_counts.mean()\nmedian_word_count = word_counts.median()\npercentile_90 = word_counts.quantile(0.9)\npercentile_95 = word_counts.quantile(0.95)\n\nprint(f\"Mean word count: {mean_word_count}\")\nprint(f\"Median word count: {median_word_count}\")\nprint(f\"90th percentile: {percentile_90}\")\nprint(f\"95th percentile: {percentile_95}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:16:30.811643Z","iopub.execute_input":"2025-04-30T09:16:30.811888Z","iopub.status.idle":"2025-04-30T09:16:30.862370Z","shell.execute_reply.started":"2025-04-30T09:16:30.811869Z","shell.execute_reply":"2025-04-30T09:16:30.861474Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count recipes with less than upper_threshold words\nupper_threshold = 300\ncount_less_threshold = (word_counts <= upper_threshold).sum()\n\nupper_percentage = count_less_threshold/len(df) * 100\n\nprint(f\"{round(upper_percentage, 2)}% of the recipes have at most {upper_threshold} words.\")","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:16:30.863428Z","iopub.execute_input":"2025-04-30T09:16:30.863695Z","iopub.status.idle":"2025-04-30T09:16:30.871223Z","shell.execute_reply.started":"2025-04-30T09:16:30.863673Z","shell.execute_reply":"2025-04-30T09:16:30.870295Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count recipes with more than lower_threshold words\nlower_threshold = 20\ncount_more_threshold = (word_counts >= lower_threshold).sum()\n\nlower_percentage = count_more_threshold/len(df) * 100\n\nprint(f\"{round(lower_percentage, 2)}% of the recipes have at least {lower_threshold} words.\")","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:16:30.872281Z","iopub.execute_input":"2025-04-30T09:16:30.872637Z","iopub.status.idle":"2025-04-30T09:16:30.903218Z","shell.execute_reply.started":"2025-04-30T09:16:30.872611Z","shell.execute_reply":"2025-04-30T09:16:30.902389Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Analyze entities distribution\nThe aim of this section is to analyse the number of elements in the NER column, which contains the number of entities in each recipe, and to provide a distribution, giving a rough measure of the \"complexity\" of the recipe.","metadata":{}},{"cell_type":"code","source":"# Define a new column containing the number of entities of each recipe\nner_column = 'ner_len'\ndf[ner_column] = df['NER'].apply(lambda x: len(ast.literal_eval(x)))","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:16:30.904497Z","iopub.execute_input":"2025-04-30T09:16:30.904841Z","iopub.status.idle":"2025-04-30T09:16:43.470771Z","shell.execute_reply.started":"2025-04-30T09:16:30.904798Z","shell.execute_reply":"2025-04-30T09:16:43.469773Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Retrieve the length and index of recipe with highest and lowest NER number\nmax_ner_len = df[ner_column].max()\nmin_ner_len = df[ner_column].min()\n\nmax_ner_len_idx = df[ner_column].idxmax()\nprint(f\"Recipes have between {min_ner_len} and {max_ner_len} entities.\")\n\nprint(f\"The 'biggest' recipe has {max_ner_len} entities and it is at location {max_ner_len_idx}.\")\n#print(df['NER'].loc[max_ner_len_idx])","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:16:43.471886Z","iopub.execute_input":"2025-04-30T09:16:43.472222Z","iopub.status.idle":"2025-04-30T09:16:43.480324Z","shell.execute_reply.started":"2025-04-30T09:16:43.472191Z","shell.execute_reply":"2025-04-30T09:16:43.479390Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize the number of entities in each recipe\nplt.figure(figsize=(12,6))\nplt.hist(df[ner_column], bins=max_ner_len, range=(0, max_ner_len))\nplt.title('Distribution of the number of entities')\nplt.xlabel('Number of entities')\nplt.ylabel('Number of recipes')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:16:43.481421Z","iopub.execute_input":"2025-04-30T09:16:43.481749Z","iopub.status.idle":"2025-04-30T09:16:44.160336Z","shell.execute_reply.started":"2025-04-30T09:16:43.481722Z","shell.execute_reply":"2025-04-30T09:16:44.159437Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Analyzing recipe NER count distribution\nentities_counts = df[ner_column].dropna()\nmean_entities_count = entities_counts.mean()\nmedian_entities_count = entities_counts.median()\npercentile_90 = entities_counts.quantile(0.9)\npercentile_95 = entities_counts.quantile(0.95)\n\nprint(f\"Mean NER count: {mean_entities_count}\")\nprint(f\"Median NER count: {median_entities_count}\")\nprint(f\"90th percentile: {percentile_90}\")\nprint(f\"95th percentile: {percentile_95}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:16:44.161375Z","iopub.execute_input":"2025-04-30T09:16:44.161717Z","iopub.status.idle":"2025-04-30T09:16:44.192665Z","shell.execute_reply.started":"2025-04-30T09:16:44.161691Z","shell.execute_reply":"2025-04-30T09:16:44.191640Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count recipes with less than upper_threshold entities\nupper_threshold = 25\ncount_less_threshold = (entities_counts <= upper_threshold).sum()\n\nupper_percentage = count_less_threshold/len(df) * 100\n\nprint(f\"{round(upper_percentage, 2)}% of the recipes have at most {upper_threshold} entities.\")","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:16:44.193520Z","iopub.execute_input":"2025-04-30T09:16:44.193797Z","iopub.status.idle":"2025-04-30T09:16:44.200734Z","shell.execute_reply.started":"2025-04-30T09:16:44.193777Z","shell.execute_reply":"2025-04-30T09:16:44.199844Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count recipes with more than lower_threshold entities\nlower_threshold = 3\ncount_more_threshold = (entities_counts >= lower_threshold).sum()\n\nlower_percentage = count_more_threshold/len(df) * 100\n\nprint(f\"{round(lower_percentage, 2)}% of the recipes have at least {lower_threshold} entities.\")","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:16:44.201680Z","iopub.execute_input":"2025-04-30T09:16:44.202163Z","iopub.status.idle":"2025-04-30T09:16:44.226290Z","shell.execute_reply.started":"2025-04-30T09:16:44.202126Z","shell.execute_reply":"2025-04-30T09:16:44.225303Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter recipes where there are NO NER\nzero_NER_recipes = df[df['ner_len'] == 0]\n\nprint(zero_NER_recipes[['title', 'NER']])\n# Drop recipes where there are no NERs\ndf = df[df['ner_len'] != 0]","metadata":{"execution":{"iopub.status.busy":"2025-04-30T09:16:44.227366Z","iopub.execute_input":"2025-04-30T09:16:44.227670Z","iopub.status.idle":"2025-04-30T09:16:44.663542Z","shell.execute_reply.started":"2025-04-30T09:16:44.227649Z","shell.execute_reply":"2025-04-30T09:16:44.662587Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It means that the NER process didn't detect any entities for these recipes. This could happen for various reasons, such as:\n- The recipe doesn't contain recognizable entities like ingredients, amounts, or special terms.\n- The NER process might not be comprehensive enough to detect all relevant entities in the recipe.\n- The recipe could be incomplete or malformed in a way that prevents proper entity recognition.\n\nFor this reason, we decided to drop these rows.\n\nNB: It may happens that there are NO entries without NER, since we are using a subsample of the original dataset. ","metadata":{}},{"cell_type":"markdown","source":"## Clustering","metadata":{}},{"cell_type":"markdown","source":"Now we want to perform some clustering analysis.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:16:44.664354Z","iopub.execute_input":"2025-04-30T09:16:44.664594Z","iopub.status.idle":"2025-04-30T09:16:44.981239Z","shell.execute_reply.started":"2025-04-30T09:16:44.664577Z","shell.execute_reply":"2025-04-30T09:16:44.980525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(stop_words='english')\ntitles = df['title'].astype(str)\nX = vectorizer.fit_transform(titles)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:16:44.982048Z","iopub.execute_input":"2025-04-30T09:16:44.982267Z","iopub.status.idle":"2025-04-30T09:16:48.413165Z","shell.execute_reply.started":"2025-04-30T09:16:44.982249Z","shell.execute_reply":"2025-04-30T09:16:48.412001Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inertia = []\nk_values = list(range(2, 50))\n\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n    kmeans.fit(X)\n    inertia.append(kmeans.inertia_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:16:48.414097Z","iopub.execute_input":"2025-04-30T09:16:48.414363Z","iopub.status.idle":"2025-04-30T09:20:38.278484Z","shell.execute_reply.started":"2025-04-30T09:16:48.414344Z","shell.execute_reply":"2025-04-30T09:20:38.277421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(16, 9))\nplt.plot(k_values, inertia, marker='o')\nplt.title('Elbow method to determine k')\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('Intertia')\nplt.xticks(k_values)\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:20:38.279738Z","iopub.execute_input":"2025-04-30T09:20:38.280362Z","iopub.status.idle":"2025-04-30T09:20:38.722495Z","shell.execute_reply.started":"2025-04-30T09:20:38.280328Z","shell.execute_reply":"2025-04-30T09:20:38.721599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It seems that two interesting values for K are 13 and 21, let's try both of them.","metadata":{}},{"cell_type":"code","source":"n_clusters = 13\nkmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\nkmeans.fit(X)\n\n# Add clusters to the original dataframe\ndf['cluster'] = kmeans.labels_\n\n# Print an example of titles grouped by cluster\nfor i in range(n_clusters):\n    print(f\"\\nCluster {i}:\")\n    print(df[df['cluster'] == i]['title'].head(5).to_string(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:20:38.723577Z","iopub.execute_input":"2025-04-30T09:20:38.723928Z","iopub.status.idle":"2025-04-30T09:20:41.401798Z","shell.execute_reply.started":"2025-04-30T09:20:38.723897Z","shell.execute_reply":"2025-04-30T09:20:41.400455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# la eliminiamo?\n# Get the centroid for the first cluster\ncentroid = kmeans.cluster_centers_[0]\n\n# Sort terms according to their weights\n# (argsort goes from lowest to highest, we reverse the order through slicing)\nsorted_terms = centroid.argsort()[::-1]\n\n# Print out the top 10 terms for the cluster\n[titles.iloc[j] for j in sorted_terms[:10]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:20:41.402831Z","iopub.execute_input":"2025-04-30T09:20:41.403178Z","iopub.status.idle":"2025-04-30T09:20:41.411312Z","shell.execute_reply.started":"2025-04-30T09:20:41.403148Z","shell.execute_reply":"2025-04-30T09:20:41.410022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Top terms per cluster:\")\nvocab = vectorizer.get_feature_names_out()\n\nfor i in range(kmeans.n_clusters):\n    centroid = kmeans.cluster_centers_[i]\n    sorted_terms = centroid.argsort()[::-1]\n    print(f\"Cluster {i}:\\t{[vocab[j] for j in sorted_terms[:5]]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Number of docs in: ')\n\nfor i in range(kmeans.n_clusters):\n    print(f\"Cluster {i}: {np.sum(kmeans.labels_ == i)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_clusters = 21\nkmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\nkmeans.fit(X)\n\ndf['cluster'] = kmeans.labels_\n\nfor i in range(n_clusters):\n    print(f\"\\nCluster {i}:\")\n    print(df[df['cluster'] == i]['title'].head(5).to_string(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:20:41.412543Z","iopub.execute_input":"2025-04-30T09:20:41.412995Z","iopub.status.idle":"2025-04-30T09:20:45.237071Z","shell.execute_reply.started":"2025-04-30T09:20:41.412962Z","shell.execute_reply":"2025-04-30T09:20:45.236115Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# la rimuoviamo?\ncentroid = kmeans.cluster_centers_[0]\n\nsorted_terms = centroid.argsort()[::-1]\n\n[titles.iloc[j] for j in sorted_terms[:20]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:20:45.238658Z","iopub.execute_input":"2025-04-30T09:20:45.239126Z","iopub.status.idle":"2025-04-30T09:20:45.247259Z","shell.execute_reply.started":"2025-04-30T09:20:45.239095Z","shell.execute_reply":"2025-04-30T09:20:45.246457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Top terms per cluster:\")\nvocab = vectorizer.get_feature_names_out()\n\nfor i in range(kmeans.n_clusters):\n    centroid = kmeans.cluster_centers_[i]\n    sorted_terms = centroid.argsort()[::-1]\n    print(f\"Cluster {i}:\\t{[vocab[j] for j in sorted_terms[:5]]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:20:45.248427Z","iopub.execute_input":"2025-04-30T09:20:45.248719Z","iopub.status.idle":"2025-04-30T09:20:45.328301Z","shell.execute_reply.started":"2025-04-30T09:20:45.248692Z","shell.execute_reply":"2025-04-30T09:20:45.327484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Number of docs in: ')\n\nfor i in range(kmeans.n_clusters):\n    print(f\"Cluster {i}: {np.sum(kmeans.labels_ == i)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:20:45.329369Z","iopub.execute_input":"2025-04-30T09:20:45.329778Z","iopub.status.idle":"2025-04-30T09:20:45.355355Z","shell.execute_reply.started":"2025-04-30T09:20:45.329747Z","shell.execute_reply":"2025-04-30T09:20:45.354059Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation\n\nIn order to evaluate our clustering we are using only intrinsic method, since we do not have the real labels of the clusters.","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics\n\nprint(\"Intrinsic evaluation measures:\")\nprint(\"Within-cluster sum-of-squares:\", str(kmeans.inertia_))\n#print(\"Silhouette coefficient:\", str(metrics.silhouette_score(X, kmeans.labels_)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:20:45.356298Z","iopub.execute_input":"2025-04-30T09:20:45.356607Z","iopub.status.idle":"2025-04-30T09:20:45.361716Z","shell.execute_reply.started":"2025-04-30T09:20:45.356581Z","shell.execute_reply":"2025-04-30T09:20:45.360898Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since the dataset is quite big, even reducing it to a quarter of the original, we try to perform also a Mini Batch Clustering.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans\n\nn_clusters = 21\nmb_kmeans = MiniBatchKMeans(n_clusters=n_clusters,batch_size=500, random_state=2307)\nmb_kmeans.fit(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:20:45.362469Z","iopub.execute_input":"2025-04-30T09:20:45.362762Z","iopub.status.idle":"2025-04-30T09:20:54.158115Z","shell.execute_reply.started":"2025-04-30T09:20:45.362735Z","shell.execute_reply":"2025-04-30T09:20:54.157278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Intrinsic evaluation measures:\")\nprint(\"Within-cluster sum-of-squares:\", str(mb_kmeans.inertia_))\n#print(\"Silhouette coefficient:\", str(metrics.silhouette_score(X, mb_kmeans.labels_)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:20:54.158982Z","iopub.execute_input":"2025-04-30T09:20:54.159291Z","iopub.status.idle":"2025-04-30T09:20:54.164514Z","shell.execute_reply.started":"2025-04-30T09:20:54.159263Z","shell.execute_reply":"2025-04-30T09:20:54.163287Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, let's try to visualize clusters.","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nsvd = TruncatedSVD(3)\nreduced_data = svd.fit_transform(X)\n\n[x,y,z] = np.transpose(reduced_data)\n[x,y,z]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:20:54.171573Z","iopub.execute_input":"2025-04-30T09:20:54.171926Z","iopub.status.idle":"2025-04-30T09:20:56.934999Z","shell.execute_reply.started":"2025-04-30T09:20:54.171898Z","shell.execute_reply":"2025-04-30T09:20:56.934108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = plt.figure(figsize=(15, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z, c=kmeans.labels_, marker='.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:20:56.935978Z","iopub.execute_input":"2025-04-30T09:20:56.936326Z","iopub.status.idle":"2025-04-30T09:21:12.858829Z","shell.execute_reply.started":"2025-04-30T09:20:56.936293Z","shell.execute_reply":"2025-04-30T09:21:12.857971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have also tried to perform clustering for the NER column, performing the 'elbow method' and plotting the results. However, the plot has not highlighter any points in which improvement in the measure decreases substantially from one time step to the next. ","metadata":{}},{"cell_type":"markdown","source":"## Indexing\nWe decided to index the recipes based on the directions, in order to perform keyword search over them. We choose the directions since they will likely contains both ingredients and keywords relative to the procedure (e.g. baked, fried).","metadata":{}},{"cell_type":"code","source":"!pip install -q python-terrier==0.11.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:21:12.860039Z","iopub.execute_input":"2025-04-30T09:21:12.860589Z","iopub.status.idle":"2025-04-30T09:21:34.390320Z","shell.execute_reply.started":"2025-04-30T09:21:12.860562Z","shell.execute_reply":"2025-04-30T09:21:34.388911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport pyterrier as pt\n\nif not pt.started():\n  pt.init()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:21:34.391700Z","iopub.execute_input":"2025-04-30T09:21:34.391991Z","iopub.status.idle":"2025-04-30T09:21:37.028593Z","shell.execute_reply.started":"2025-04-30T09:21:34.391963Z","shell.execute_reply":"2025-04-30T09:21:37.027363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a list of documents (use 'title' for indexing)\ndocuments_title = [{'docno': str(i), 'text': text} for i, text in enumerate(df['title'])]\n\n# Create the index\nindexer = pt.IterDictIndexer(\"./index_title\")\nindexer.index(documents_title)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:21:37.029709Z","iopub.execute_input":"2025-04-30T09:21:37.030327Z","iopub.status.idle":"2025-04-30T09:22:05.217541Z","shell.execute_reply.started":"2025-04-30T09:21:37.030295Z","shell.execute_reply":"2025-04-30T09:22:05.216917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a list of documents (use 'directions' for indexing)\ndocuments_directions = [{'docno': str(i), 'text': text} for i, text in enumerate(df['directions'])]\n\n# Create the index\nindexer = pt.IterDictIndexer(\"./index_directions\")\nindexer.index(documents_directions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:22:05.218272Z","iopub.execute_input":"2025-04-30T09:22:05.218568Z","iopub.status.idle":"2025-04-30T09:23:39.735536Z","shell.execute_reply.started":"2025-04-30T09:22:05.218547Z","shell.execute_reply":"2025-04-30T09:23:39.734384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create documents with multiple fields\ndocuments_fields = [\n    {\n        'docno': str(i),\n        'title': row['title'],\n        'ingredients': row['ingredients'],\n        'directions': row['directions']\n    }\n    for i, row in df.iterrows()\n]\n\n# Index the fielded documents\nindexer_fields = pt.IterDictIndexer(\"./index_fields\")\n\n# Set meta fields and indexed fields\nindexref = indexer_fields.index(\n    documents_fields,\n    fields=[\"title\", \"ingredients\", \"directions\"],  \n    meta={'docno': 20, 'title': 512, 'ingredients': 1024, 'directions': 4096}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:23:39.736958Z","iopub.execute_input":"2025-04-30T09:23:39.737315Z","iopub.status.idle":"2025-04-30T09:27:15.121023Z","shell.execute_reply.started":"2025-04-30T09:23:39.737287Z","shell.execute_reply":"2025-04-30T09:27:15.119483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a searcher using the index\nindex_title = pt.IndexFactory.of(\"./index_title\")\nindex_directions = pt.IndexFactory.of(\"./index_directions\")\nindex_fields = pt.IndexFactory.of(indexref)\n\n# BM25 retrieval model\nbm25_tit = pt.terrier.Retriever(index_title, wmodel=\"BM25\")\nbm25_dir = pt.terrier.Retriever(index_directions, wmodel=\"BM25\")\n\n# TF-IDF retrieval model\ntfidf_tit = pt.terrier.Retriever(index_title, wmodel=\"TF_IDF\")\ntfidf_dir = pt.terrier.Retriever(index_directions, wmodel=\"TF_IDF\")\n\n# DFRee retrieval model (Document Frequency based)\ndfree_tit = pt.terrier.Retriever(index_title, wmodel=\"DFRee\")\ndfree_dir = pt.terrier.Retriever(index_directions, wmodel=\"DFRee\")\n\n# Create BM25F retriever with field weights\n# Weighted BM25 over each field\nbm25_title = pt.BatchRetrieve(index_fields, wmodel=\"BM25\", controls={\"w\": \"1.0\"}, metadata=[\"docno\", \"title\"], field=\"title\")\nbm25_ingredients = pt.BatchRetrieve(index_fields, wmodel=\"BM25\", controls={\"w\": \"1.0\"}, metadata=[\"docno\", \"ingredients\"], field=\"ingredients\")\nbm25_directions = pt.BatchRetrieve(index_fields, wmodel=\"BM25\", controls={\"w\": \"1.0\"}, metadata=[\"docno\", \"directions\"], field=\"directions\")\n\n# Weighted combination of scores: BM25F-like\nbm25f_manual = (\n    bm25_title * 0.3 +\n    bm25_ingredients * 0.4 +\n    bm25_directions * 0.3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:27:15.122596Z","iopub.execute_input":"2025-04-30T09:27:15.123025Z","iopub.status.idle":"2025-04-30T09:27:15.216972Z","shell.execute_reply.started":"2025-04-30T09:27:15.122992Z","shell.execute_reply":"2025-04-30T09:27:15.214886Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"By using all of these models, we can perform a comprehensive evaluation of the different retrieval strategies. Each model captures different aspects of relevance:\n\n- BM25 accounts for both term and document frequencies, providing a balanced relevance score.\n- TF-IDF emphasizes the importance of rare terms across the document collection.\n- DFRee is useful for understanding how document frequency affects relevance.\n\nWe also tried BM25 without term frequency (no tf), but it performed to same as classic BM25. This is likely because in our dataset, document length and term frequency did not vary significantly across relevant documents, so removing the tf component had minimal impact on ranking.\nTo keep the notebook clean we omitted this variant.","metadata":{}},{"cell_type":"code","source":"print(index_title.getCollectionStatistics().toString())\nprint(index_directions.getCollectionStatistics().toString())\nprint(index_fields.getCollectionStatistics().toString())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:27:15.218230Z","iopub.execute_input":"2025-04-30T09:27:15.220307Z","iopub.status.idle":"2025-04-30T09:27:15.245468Z","shell.execute_reply.started":"2025-04-30T09:27:15.220273Z","shell.execute_reply":"2025-04-30T09:27:15.244356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare some queries and evaluate them\nqueries = [\n    {\"qid\": \"1\", \"query\": \"chicken casserole\"},\n    {\"qid\": \"2\", \"query\": \"simple birthday cake\"},\n    {\"qid\": \"3\", \"query\": \"baked salmon\"},\n]\n\ntitles = df['title']\nplot_data = []\n\n# Loop through each query\nfor q in queries:\n    print(f\"\\n=== Query {q['qid']}: {q['query']} ===\")\n\n    # Run all models for the titles including BM25F\n    result_bm25_tit = bm25_tit.search(q['query'])\n    result_tfidf_tit = tfidf_tit.search(q['query'])\n    result_dfree_tit = dfree_tit.search(q['query'])   \n\n    # Add all model results to the loop\n    for method_name, result in [\n        (\"BM25\", result_bm25_tit),\n        (\"TF-IDF\", result_tfidf_tit),\n        (\"DFRee\", result_dfree_tit),\n    ]:\n        print(f\"\\n--- Method: {method_name} ---\")\n\n        for rank, (docno, score) in enumerate(zip(result[\"docno\"][:5], result[\"score\"][:5])):\n            docno_int = int(docno)  # Convert from str to int\n            title = df['title'].iloc[docno_int] if docno_int < len(df) else \"TITLE NOT FOUND\"\n            print(f\"DocNO: {docno:<7} | Title: {title:<50.48} | Score: {score:.4f}\")\n            \n            # Append to plotting data\n            plot_data.append({\n                'Query': q['query'],\n                'Query_ID': q['qid'],\n                'Model': method_name,\n                'Rank': rank + 1,\n                'Docno': docno_int,\n                'Title': title,\n                'Score': score\n            })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:34:04.953338Z","iopub.execute_input":"2025-04-30T09:34:04.954583Z","iopub.status.idle":"2025-04-30T09:34:05.285560Z","shell.execute_reply.started":"2025-04-30T09:34:04.954550Z","shell.execute_reply":"2025-04-30T09:34:05.284626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize the results of the third query\nplot_df = pd.DataFrame(plot_data)\ndf_first = plot_df[plot_df['Query_ID'] == \"3\"]\n\nplt.figure(figsize=(10, 6))\n\nfor model in df_first['Model'].unique():\n    md = df_first[df_first['Model'] == model]\n    plt.plot(md['Rank'], md['Title'], marker='o', linestyle='-', label=model)\n\nplt.xlabel('Rank')\nplt.ylabel('Recipe title')\nplt.title(f'Results for Query \"{queries[2][\"query\"]}\"')\nplt.legend(title='Model')\nplt.xticks([1, 2, 3, 4, 5])\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:34:12.241238Z","iopub.execute_input":"2025-04-30T09:34:12.241582Z","iopub.status.idle":"2025-04-30T09:34:12.513144Z","shell.execute_reply.started":"2025-04-30T09:34:12.241559Z","shell.execute_reply":"2025-04-30T09:34:12.512167Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this case, BM25 and TF-IDF yielded the same document rankings: this happen when documents are short, uniformly sized, or when term frequencies are relatively balanced, making the additional complexity of BM25 less impactful.\n\nThis suggest that relying only on document titles for indexing provides limited term variability and context, reducing the effectiveness of retrieval models. Incorporating richer textual content would likely yield more discriminative and informative rankings.","metadata":{}},{"cell_type":"code","source":"# Prepare some queries and evaluate them\nqueries = [\n    {\"qid\": \"1\", \"query\": \"chicken casserole\"},\n    {\"qid\": \"2\", \"query\": \"simple birthday cake\"},\n    {\"qid\": \"3\", \"query\": \"baked salmon\"},\n]\n\ntitles = df['title']\nplot_data = []\n\n# Loop through each query\nfor q in queries:\n    print(f\"\\n=== Query {q['qid']}: {q['query']} ===\")\n\n    # Run all models for the directions including BM25F\n    result_bm25_dir = bm25_dir.search(q['query'])\n    result_tfidf_dir = tfidf_dir.search(q['query'])\n    result_dfree_dir = dfree_dir.search(q['query'])\n    \n    result_bm25f = bm25f_manual.search(q['query'])  \n\n    # Add all model results to the loop\n    for method_name, result in [\n        (\"BM25\", result_bm25_dir),\n        (\"TF-IDF\", result_tfidf_dir),\n        (\"DFRee\", result_dfree_dir),\n        (\"BM25F\", result_bm25f) \n    ]:\n        print(f\"\\n--- Method: {method_name} ---\")\n\n        for rank, (docno, score) in enumerate(zip(result[\"docno\"][:5], result[\"score\"][:5])):\n            docno_int = int(docno)  # Convert from str to int\n            title = df['title'].iloc[docno_int] if docno_int < len(df) else \"TITLE NOT FOUND\"\n            print(f\"DocNO: {docno:<7} | Title: {title:<50.48} | Score: {score:.4f}\")\n            \n            # Append to plotting data\n            plot_data.append({\n                'Query': q['query'],\n                'Query_ID': q['qid'],\n                'Model': method_name,\n                'Rank': rank + 1,\n                'Docno': docno_int,\n                'Title': title,\n                'Score': score\n            })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:34:22.157772Z","iopub.execute_input":"2025-04-30T09:34:22.158170Z","iopub.status.idle":"2025-04-30T09:34:23.576412Z","shell.execute_reply.started":"2025-04-30T09:34:22.158144Z","shell.execute_reply":"2025-04-30T09:34:23.575357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize the results of the third query\nplot_df = pd.DataFrame(plot_data)\ndf_first = plot_df[plot_df['Query_ID'] == \"3\"]\n\nplt.figure(figsize=(10, 6))\n\nfor model in df_first['Model'].unique():\n    md = df_first[df_first['Model'] == model]\n    plt.plot(md['Rank'], md['Title'], marker='o', linestyle='-', label=model)\n\nplt.xlabel('Rank')\nplt.ylabel('Recipe title')\nplt.title(f'Results for Query \"{queries[2][\"query\"]}\"')\nplt.legend(title='Model')\nplt.xticks([1, 2, 3, 4, 5])\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:34:27.667197Z","iopub.execute_input":"2025-04-30T09:34:27.667548Z","iopub.status.idle":"2025-04-30T09:34:27.948252Z","shell.execute_reply.started":"2025-04-30T09:34:27.667522Z","shell.execute_reply":"2025-04-30T09:34:27.947226Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"About the results:\n\n- TF-IDF put “Balsamic Glazed Salmon” at #1: we can see that TF-IDF’s emphasis on rare terms may be boosting “balsamic”.\n- DFRee puts “Fennel-And-Dill-Rubbed Grilled Salmon” at #1, and then “Salmon Wrapped In Fig Leaves With Baked Kale” at #2: it shows that rarer, longer titles win more weight under DFRee.\n- Unlike single-field models, BM25F combines weighted evidence from different parts of the document, making it robust when key terms appear in both title, ingredients and directions.","metadata":{}},{"cell_type":"markdown","source":"## Word2Vec","metadata":{}},{"cell_type":"markdown","source":"Training a Word2Vec embedding on the RecipeNLG dataset allows us to capture the semantic relationships between words in cooking instructions. Word2Vec learns to represent words as dense vectors, where words with similar meanings are positioned closer together in the vector space. This enables us to explore word similarities, identify common patterns, and improve various natural language processing tasks like recipe recommendation and ingredient analysis.","metadata":{}},{"cell_type":"code","source":"# Convert the 'text' column of the DataFrame into a list of strings (e.g., recipe directions)\nw2v_directions = list(df['text'])\n\n# Tokenize each direction \ntokenized_directions = [re.sub('\\W', ' ', direction).lower().split() for direction in w2v_directions]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:51:57.372682Z","iopub.execute_input":"2025-04-30T09:51:57.373070Z","iopub.status.idle":"2025-04-30T09:52:39.819752Z","shell.execute_reply.started":"2025-04-30T09:51:57.373024Z","shell.execute_reply":"2025-04-30T09:52:39.818774Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's remove the stopwords\n(we need a context window to train it, so we are discarding the directions composed by only one word)","metadata":{}},{"cell_type":"code","source":"filtered_directions = [[word for word in sublist if word not in stop_words] for sublist in tokenized_directions]\nfiltered_directions = [direction for direction in filtered_directions if len(direction) > 1]\nlen(filtered_directions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:53:53.291039Z","iopub.execute_input":"2025-04-30T09:53:53.291349Z","iopub.status.idle":"2025-04-30T09:54:03.676739Z","shell.execute_reply.started":"2025-04-30T09:53:53.291328Z","shell.execute_reply":"2025-04-30T09:54:03.675794Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now it's time to  train our model!\n","metadata":{}},{"cell_type":"code","source":"from gensim.models.word2vec import Word2Vec\n\nmodel = Word2Vec(filtered_directions, vector_size=50, min_count=5, window=10)\n\nlen(model.wv)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:54:16.641020Z","iopub.execute_input":"2025-04-30T09:54:16.642285Z","iopub.status.idle":"2025-04-30T09:57:10.959520Z","shell.execute_reply.started":"2025-04-30T09:54:16.642248Z","shell.execute_reply":"2025-04-30T09:57:10.958679Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Ever wondered what is a possible vector representation of the word soup?  Me neither!","metadata":{}},{"cell_type":"code","source":"term = 'soup'\nmodel.wv[term]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:58:10.460612Z","iopub.execute_input":"2025-04-30T09:58:10.460967Z","iopub.status.idle":"2025-04-30T09:58:10.469315Z","shell.execute_reply.started":"2025-04-30T09:58:10.460939Z","shell.execute_reply":"2025-04-30T09:58:10.468438Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Or what are the most similar words to 'chicken' (Between the ones in our dataset)","metadata":{}},{"cell_type":"code","source":"term = 'chicken'\nmodel.wv.most_similar(term)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:58:10.501553Z","iopub.execute_input":"2025-04-30T09:58:10.501880Z","iopub.status.idle":"2025-04-30T09:58:10.512706Z","shell.execute_reply.started":"2025-04-30T09:58:10.501856Z","shell.execute_reply":"2025-04-30T09:58:10.511887Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From 0 to 1, how  similar the words chicken and beef are?","metadata":{}},{"cell_type":"code","source":"model.wv.similarity('chicken', 'beef')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:58:10.535564Z","iopub.execute_input":"2025-04-30T09:58:10.536001Z","iopub.status.idle":"2025-04-30T09:58:10.543152Z","shell.execute_reply.started":"2025-04-30T09:58:10.535973Z","shell.execute_reply":"2025-04-30T09:58:10.542158Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"chicken:breast = beef:?","metadata":{}},{"cell_type":"code","source":"vec = model.wv['beef'] + model.wv['breast'] - model.wv['chicken']\nmodel.wv.similar_by_vector(vec)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:58:10.569643Z","iopub.execute_input":"2025-04-30T09:58:10.570010Z","iopub.status.idle":"2025-04-30T09:58:10.578739Z","shell.execute_reply.started":"2025-04-30T09:58:10.569982Z","shell.execute_reply":"2025-04-30T09:58:10.577908Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Right now we are working with embedding vectors of 50 dimensions. Unfortunately, the human brain is limited and can visualize only up to three of them. So it's necessary to reduce our vectors' dimensions to this number. Let's try to do so using t-distributed stochastic neighbor embedding (t-SNE):","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n\nsamples = random.sample(list(model.wv.key_to_index), 1000)\nvectors = model.wv[samples]\n\ntsne = TSNE(n_components=3, n_iter=2000)\ntsne_embedding = tsne.fit_transform(vectors)\n\nx, y, z = np.transpose(tsne_embedding)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:58:10.592441Z","iopub.execute_input":"2025-04-30T09:58:10.592755Z","iopub.status.idle":"2025-04-30T09:58:18.687046Z","shell.execute_reply.started":"2025-04-30T09:58:10.592734Z","shell.execute_reply":"2025-04-30T09:58:18.686337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.express as px\n\nfig = px.scatter_3d(x=x[:300],y=y[:300],z=z[:300],text=samples[:300])\nfig.update_traces(marker=dict(size=3,line=dict(width=2)),textfont_size=10)\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:58:18.688618Z","iopub.execute_input":"2025-04-30T09:58:18.689539Z","iopub.status.idle":"2025-04-30T09:58:18.765524Z","shell.execute_reply.started":"2025-04-30T09:58:18.689509Z","shell.execute_reply":"2025-04-30T09:58:18.764306Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Quite interesting isn't it? \n\nNumbers are near each other (probably they represent different oven temperatures). Also values like 9x5x3 and 2x3 (lengths of the baking trays?).\n\nSame for fruits and vegetables","metadata":{}},{"cell_type":"markdown","source":"Why don't we try principal component analysis to perform the dimensionality reduction?\n\n(and also sample again, you never know)","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:58:18.766377Z","iopub.execute_input":"2025-04-30T09:58:18.766697Z","iopub.status.idle":"2025-04-30T09:58:18.771151Z","shell.execute_reply.started":"2025-04-30T09:58:18.766672Z","shell.execute_reply":"2025-04-30T09:58:18.770290Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"samples = random.sample(list(model.wv.key_to_index), 1000)\nvectors = model.wv[samples]\n\npca = PCA(n_components=3)\npca_embedding = pca.fit_transform(vectors)\n\nexplained_variance_ratio = pca.explained_variance_ratio_\nprint(f\"Explained variance ratio: {explained_variance_ratio}\" )\nprint(f\"Total explained variance: {sum(explained_variance_ratio)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:58:18.772781Z","iopub.execute_input":"2025-04-30T09:58:18.773091Z","iopub.status.idle":"2025-04-30T09:58:18.814010Z","shell.execute_reply.started":"2025-04-30T09:58:18.773064Z","shell.execute_reply":"2025-04-30T09:58:18.812280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x, y, z = np.transpose(pca_embedding)\n\nfig = px.scatter_3d(x=x[:200],y=y[:200],z=z[:200],text=samples[:200])\nfig.update_traces(marker=dict(size=3,line=dict(width=2)),textfont_size=10)\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T09:58:18.814733Z","iopub.execute_input":"2025-04-30T09:58:18.815007Z","iopub.status.idle":"2025-04-30T09:58:18.890560Z","shell.execute_reply.started":"2025-04-30T09:58:18.814986Z","shell.execute_reply":"2025-04-30T09:58:18.889688Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The 3 dimensions that hold the most informations tent to agglomerate around the values (0,0,0), making pca reductions not suited for visualisation purposes\n","metadata":{}},{"cell_type":"markdown","source":"# Training models","metadata":{}},{"cell_type":"markdown","source":"In this section we are going to train some models, from the easiest to the most complex ones.\n\nFirstly, we create a secreto token to use HuggingFace models.","metadata":{}},{"cell_type":"code","source":"⁠from kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"NLP project\") ⁠\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Further development ","metadata":{}},{"cell_type":"code","source":"We would like to see how some of our riceipts can look like...","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract 3 titles to turn into images\nsampled_titles = df['title'].sample(n=3, random_state=42).tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom huggingface_hub import login\n\nlogin(secret_value_0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\npipe.to(\"cuda\")\n\n# if using torch < 2.0\n# pipe.enable_xformers_memory_efficient_attention()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(15, 5))\nfor i, title in enumerate(sampled_titles):\n    image = pipe(prompt=title).images[0]\n    axes[i].imshow(image)\n    axes[i].set_title(title, fontsize=10)\n    axes[i].axis('off')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
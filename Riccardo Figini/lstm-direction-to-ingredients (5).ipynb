{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11601525,"sourceType":"datasetVersion","datasetId":7276106}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T10:56:40.037242Z","iopub.execute_input":"2025-05-17T10:56:40.037506Z","iopub.status.idle":"2025-05-17T10:57:54.540384Z","shell.execute_reply.started":"2025-05-17T10:56:40.037483Z","shell.execute_reply":"2025-05-17T10:57:54.539433Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T10:57:54.541742Z","iopub.execute_input":"2025-05-17T10:57:54.541989Z","iopub.status.idle":"2025-05-17T10:57:54.546102Z","shell.execute_reply.started":"2025-05-17T10:57:54.541967Z","shell.execute_reply":"2025-05-17T10:57:54.545574Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom datasets import Dataset\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt')  \n\n\ndf = pd.read_csv(\"/kaggle/input/recipe-sampled-0-25/sampled_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T10:57:54.546832Z","iopub.execute_input":"2025-05-17T10:57:54.547072Z","iopub.status.idle":"2025-05-17T10:58:16.562628Z","shell.execute_reply.started":"2025-05-17T10:57:54.547050Z","shell.execute_reply":"2025-05-17T10:58:16.561990Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"#### Preparazione del dataset","metadata":{}},{"cell_type":"code","source":"df_sample = df[[\"directions\", \"ingredients\"]].sample(n=100000, random_state=42).reset_index(drop=True)\ndf_sample.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T10:58:16.564676Z","iopub.execute_input":"2025-05-17T10:58:16.564924Z","iopub.status.idle":"2025-05-17T10:58:16.675017Z","shell.execute_reply.started":"2025-05-17T10:58:16.564905Z","shell.execute_reply":"2025-05-17T10:58:16.674439Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                          directions  \\\n0  ['Mix together the cheese, olives, onion, drie...   \n1  ['Brown meat; drain and set aside.', 'Blend ma...   \n2  ['Dissolve jello in boiling water.', 'Let cool...   \n\n                                         ingredients  \n0  [\"1 cup shredded cheddar cheese\", \"1 cup chopp...  \n1  [\"1 pie crust\", \"1/2 lb. ground beef (you can ...  \n2  [\"2 small orange jello\", \"2 c. boiling water\",...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>directions</th>\n      <th>ingredients</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>['Mix together the cheese, olives, onion, drie...</td>\n      <td>[\"1 cup shredded cheddar cheese\", \"1 cup chopp...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>['Brown meat; drain and set aside.', 'Blend ma...</td>\n      <td>[\"1 pie crust\", \"1/2 lb. ground beef (you can ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>['Dissolve jello in boiling water.', 'Let cool...</td>\n      <td>[\"2 small orange jello\", \"2 c. boiling water\",...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import ast\n\ndf_sample[\"ingredients\"] = df_sample[\"ingredients\"].apply(ast.literal_eval)\ndf_sample[\"directions\"] = df_sample[\"directions\"].apply(ast.literal_eval)\n\nprint(type(df_sample.loc[0, \"ingredients\"]))  # deve essere <class 'list'>\nprint(df_sample.loc[0, \"ingredients\"])        # stampa la lista vera\n\nprint(type(df_sample.loc[0, \"directions\"]))  # deve essere <class 'list'>\nprint(df_sample.loc[0, \"directions\"])        # stampa la lista vera","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T10:58:16.675738Z","iopub.execute_input":"2025-05-17T10:58:16.676014Z","iopub.status.idle":"2025-05-17T10:58:20.957836Z","shell.execute_reply.started":"2025-05-17T10:58:16.675990Z","shell.execute_reply":"2025-05-17T10:58:20.957073Z"}},"outputs":[{"name":"stdout","text":"<class 'list'>\n['1 cup shredded cheddar cheese', '1 cup chopped pimento stuffed olive', '1 tablespoon minced onion', '1 cup dried beef, chopped', '3/4 - 1 cup mayonnaise', '1 loaf sliced rye cocktail bread']\n<class 'list'>\n['Mix together the cheese, olives, onion, dried beef and mayo.', 'Spread on slices of rye cocktail bread. place the slices on a cookie sheet and broil until bubbly.']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df_sample[\"text\"] = df_sample[\"directions\"].apply(lambda steps: \" \".join(steps))\nprint(df_sample.loc[0, \"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T10:58:20.958671Z","iopub.execute_input":"2025-05-17T10:58:20.958930Z","iopub.status.idle":"2025-05-17T10:58:21.032283Z","shell.execute_reply.started":"2025-05-17T10:58:20.958905Z","shell.execute_reply":"2025-05-17T10:58:21.031451Z"}},"outputs":[{"name":"stdout","text":"Mix together the cheese, olives, onion, dried beef and mayo. Spread on slices of rye cocktail bread. place the slices on a cookie sheet and broil until bubbly.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Queste regole regex non sono complete, non coprono tutti i casi. Bisognerebbe aggiunge man mano, ma Ã¨ una operazione complicata","metadata":{}},{"cell_type":"code","source":"import re\n\ndef clean_ingredient(ingredient):\n    # Remove fractions and numbers (e.g., \"1\", \"1/2\", \"2.5\")\n    ingredient = re.sub(r'\\b\\d+([\\/\\.]\\d+)?\\b', '', ingredient)\n\n    # Common measurement units to remove\n    units = [\n        \"teaspoons?\", \"tsp\", \"tablespoons?\", \"tbsp\", \"cups?\", \"ounces?\", \"oz\",\n        \"pounds?\", \"lb\", \"grams?\", \"g\", \"kilograms?\", \"kg\", \"milliliters?\", \"ml\",\n        \"liters?\", \"l\", \"pinch\", \"clove\", \"cloves\", \"slices?\", \"dash\", \"cans?\", \n        \"packages?\", \"bunch\", \"stalks?\", \"heads?\", \"pieces?\", \"sticks?\", \"inches?\"\n    ]\n    units_pattern = r'\\b(?:' + '|'.join(units) + r')\\b'\n    ingredient = re.sub(units_pattern, '', ingredient, flags=re.IGNORECASE)\n\n    ingredient = re.sub(r'\\b(c\\.|c)\\b\\.?', '', ingredient, flags=re.IGNORECASE)\n\n    ingredient = re.sub(r'\\(\\s*\\.\\s*\\)', '', ingredient)\n    ingredient = re.sub(r'\\([^)]*\\)', '', ingredient)\n\n    ingredient = re.sub(r'\\bof\\b', '', ingredient, flags=re.IGNORECASE)\n    ingredient = re.sub(r'^\\s*\\.\\s*', '', ingredient)       # punto iniziale con spazio\n    ingredient = re.sub(r'\\.\\s*', ' ', ingredient)          # ogni \". \" ovunque\n    ingredient = re.sub(r',.*', '', ingredient)             # rimuove note dopo virgola\n\n    ingredient = re.sub(r'\\b(to |pt |pkg |qt )\\.?\\b', '', ingredient, flags=re.IGNORECASE)\n    ingredient = re.sub(r'^to\\s+', '', ingredient, flags=re.IGNORECASE)\n\n    # Remove extra spaces\n    ingredient = re.sub(r'\\s+', ' ', ingredient).strip()\n\n    return ingredient\n\n\n# Applica a tutta la colonna ingredients\ndf_sample[\"clean_ingredients\"] = df_sample[\"ingredients\"].apply(lambda lst: [clean_ingredient(i) for i in lst])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T10:58:21.033290Z","iopub.execute_input":"2025-05-17T10:58:21.033592Z","iopub.status.idle":"2025-05-17T10:58:35.113013Z","shell.execute_reply.started":"2025-05-17T10:58:21.033567Z","shell.execute_reply":"2025-05-17T10:58:35.112452Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(df_sample.loc[:10, \"clean_ingredients\"]) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T10:58:35.113704Z","iopub.execute_input":"2025-05-17T10:58:35.113929Z","iopub.status.idle":"2025-05-17T10:58:35.119808Z","shell.execute_reply.started":"2025-05-17T10:58:35.113904Z","shell.execute_reply":"2025-05-17T10:58:35.118946Z"}},"outputs":[{"name":"stdout","text":"0     [shredded cheddar cheese, chopped pimento stuf...\n1     [pie crust, ground beef, mayonnaise, milk, egg...\n2     [small orange jello, boiling water, small crus...\n3     [square graham crackers, reduced calorie marga...\n4     [cream cheese, sm jar Old English cheese, Lipt...\n5     [MIRACLE WHIP Dressing, BREAKSTONE'S or KNUDSE...\n6     [FOR THE FILLING:, Fresh Strawberries, - Fresh...\n7     [doz mangos, cabbage, celery, brown sugar, sal...\n8     [chopped green peppers, chopped red peppers, c...\n9     [fryer, uncooked rice, cream chicken soup, dry...\n10    [yeast, bread flour, salt, sugar, olive oil, w...\nName: clean_ingredients, dtype: object\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Assegno le label \"0\" \"I-food\" \"B-food\"","metadata":{}},{"cell_type":"code","source":"\ndef check(labels):\n      # ğŸ” Verifica coerenza: nessun I-FOOD senza un B-FOOD prima\n    for i, label in enumerate(labels):\n        if label == 'I-FOOD':\n            if i == 0 or labels[i - 1] not in ['B-FOOD', 'I-FOOD']:\n                raise ValueError(f\"Incoerenza IOB: I-FOOD alla posizione {i} senza B-FOOD precedente.\")\n\n\n\ndef iob_tag_tokens(text, ingredient_list):\n    tokens = word_tokenize(text)\n    labels = ['O'] * len(tokens)\n    \n    for ingredient in ingredient_list:\n        ingredient_tokens = word_tokenize(ingredient)\n        ingredient_len = len(ingredient_tokens)\n\n        if ingredient_len == 0:\n            continue  # ignora ingredienti vuoti\n\n        for i in range(len(tokens) - ingredient_len + 1):\n            window = tokens[i:i + ingredient_len]\n            if [t.lower() for t in window] == [t.lower() for t in ingredient_tokens]:\n                labels[i] = 'B-FOOD'\n                for j in range(1, ingredient_len):\n                    if i + j < len(labels):\n                        labels[i + j] = 'I-FOOD'\n                break  # evita doppi match dello stesso ingrediente\n\n\n    check(labels)\n    \n    return tokens, labels\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T10:58:35.120680Z","iopub.execute_input":"2025-05-17T10:58:35.120889Z","iopub.status.idle":"2025-05-17T10:58:35.161527Z","shell.execute_reply.started":"2025-05-17T10:58:35.120874Z","shell.execute_reply":"2025-05-17T10:58:35.160812Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df_sample[\"ner_tokens_labels\"] = df_sample.apply(\n    lambda row: iob_tag_tokens(row[\"text\"], row[\"clean_ingredients\"]), axis=1\n)\n\"\"\"\nESEMPIO UTILIZZO:\ntext = \"Aggiungi una cipolla tritata e soffriggi in olio.\"\nclean_ingredients = [\"cipolla\", \"olio\"]\n\ntokens = [\"Aggiungi\", \"una\", \"cipolla\", \"tritata\", \"e\", \"soffriggi\", \"in\", \"olio\", \".\"]\nlabels = [\"O\",        \"O\",   \"B-FOOD\", \"I-FOOD\",  \"O\", \"O\",         \"O\", \"B-FOOD\", \"O\"]\n\nRISULTATO FINALE:\n(\"Aggiungi\", ..., \"olio\", \".\"), [\"O\", ..., \"B-FOOD\", \"O\"]\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T10:58:35.164250Z","iopub.execute_input":"2025-05-17T10:58:35.164692Z","iopub.status.idle":"2025-05-17T11:01:13.155919Z","shell.execute_reply.started":"2025-05-17T10:58:35.164675Z","shell.execute_reply":"2025-05-17T11:01:13.155277Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'\\nESEMPIO UTILIZZO:\\ntext = \"Aggiungi una cipolla tritata e soffriggi in olio.\"\\nclean_ingredients = [\"cipolla\", \"olio\"]\\n\\ntokens = [\"Aggiungi\", \"una\", \"cipolla\", \"tritata\", \"e\", \"soffriggi\", \"in\", \"olio\", \".\"]\\nlabels = [\"O\",        \"O\",   \"B-FOOD\", \"I-FOOD\",  \"O\", \"O\",         \"O\", \"B-FOOD\", \"O\"]\\n\\nRISULTATO FINALE:\\n(\"Aggiungi\", ..., \"olio\", \".\"), [\"O\", ..., \"B-FOOD\", \"O\"]\\n'"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"tokens, labels = df_sample.loc[0, \"ner_tokens_labels\"]\nfor t, l in zip(tokens, labels):\n    print(f\"{t:15} â†’ {l}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T11:01:13.156619Z","iopub.execute_input":"2025-05-17T11:01:13.156877Z","iopub.status.idle":"2025-05-17T11:01:13.161822Z","shell.execute_reply.started":"2025-05-17T11:01:13.156852Z","shell.execute_reply":"2025-05-17T11:01:13.161290Z"}},"outputs":[{"name":"stdout","text":"Mix             â†’ O\ntogether        â†’ O\nthe             â†’ O\ncheese          â†’ O\n,               â†’ O\nolives          â†’ O\n,               â†’ O\nonion           â†’ O\n,               â†’ O\ndried           â†’ B-FOOD\nbeef            â†’ I-FOOD\nand             â†’ O\nmayo            â†’ O\n.               â†’ O\nSpread          â†’ O\non              â†’ O\nslices          â†’ O\nof              â†’ O\nrye             â†’ O\ncocktail        â†’ O\nbread           â†’ O\n.               â†’ O\nplace           â†’ O\nthe             â†’ O\nslices          â†’ O\non              â†’ O\na               â†’ O\ncookie          â†’ O\nsheet           â†’ O\nand             â†’ O\nbroil           â†’ O\nuntil           â†’ O\nbubbly          â†’ O\n.               â†’ O\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"#### Preparazione dell'addestramento","metadata":{}},{"cell_type":"code","source":"!pip install seqeval\n\nfrom datasets import Dataset, ClassLabel\nfrom transformers import AutoTokenizer\nfrom seqeval.metrics import precision_score, recall_score, f1_score\nfrom transformers import DataCollatorForTokenClassification\nfrom transformers import EarlyStoppingCallback\nfrom transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\nfrom seqeval.metrics import precision_score, recall_score, f1_score\nfrom torch.nn import CrossEntropyLoss\nfrom collections import Counter\nimport torch.nn as nn\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T11:01:13.162583Z","iopub.execute_input":"2025-05-17T11:01:13.162747Z","iopub.status.idle":"2025-05-17T11:01:39.357722Z","shell.execute_reply.started":"2025-05-17T11:01:13.162734Z","shell.execute_reply":"2025-05-17T11:01:39.356926Z"}},"outputs":[{"name":"stdout","text":"Collecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=c5fe456d3a01b10be7547e6557e641e2a21fe64cdca447048fd7a79d29591b88\n  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"},{"name":"stderr","text":"2025-05-17 11:01:20.187610: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747479680.386737      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747479680.443769      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"\"\"\"\n([\"Aggiungi\", \"una\", \"cipolla\", \"tritata\", \"finemente\", ...],\n [\"O\",       \"O\",   \"B-FOOD\", \"I-FOOD\", \"O\", ...])\nDIVENTA:\n{\n    \"tokens\": [\"Aggiungi\", \"una\", \"cipolla\", \"tritata\", \"finemente\", ...],\n    \"ner_tags\": [\"O\", \"O\", \"B-FOOD\", \"I-FOOD\", \"O\", ...]\n}\n\"\"\"\n\nhf_data = [\n    {\n        \"tokens\": tokens,\n        \"ner_tags\": labels\n    }\n    for tokens, labels in df_sample[\"ner_tokens_labels\"]\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T11:01:39.358656Z","iopub.execute_input":"2025-05-17T11:01:39.359275Z","iopub.status.idle":"2025-05-17T11:01:40.314783Z","shell.execute_reply.started":"2025-05-17T11:01:39.359248Z","shell.execute_reply":"2025-05-17T11:01:40.314215Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"unique_tags = set(tag for row in hf_data for tag in row[\"ner_tags\"])\nunique_tags = sorted(unique_tags)\n\ntag2id = {tag: i for i, tag in enumerate(unique_tags)}\nid2tag = {i: tag for tag, i in tag2id.items()}\n\"\"\"\ntag2id = {\"B-FOOD\": 0, \"I-FOOD\": 1, \"O\": 2}\nid2tag = {0: \"B-FOOD\", 1: \"I-FOOD\", 2: \"O\"}\n\"\"\"\n\n#Sostituisce \"ner_tags\" con una nuova chiave \"labels\" contenente gli ID\nfor row in hf_data:\n    row[\"labels\"] = [tag2id[tag] for tag in row[\"ner_tags\"]]\n    del row[\"ner_tags\"]  \n\n#Conversione in un effetivo dataset\ndataset = Dataset.from_list(hf_data)\ndataset = dataset.train_test_split(test_size=0.2, seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T11:01:40.315529Z","iopub.execute_input":"2025-05-17T11:01:40.315753Z","iopub.status.idle":"2025-05-17T11:01:44.947800Z","shell.execute_reply.started":"2025-05-17T11:01:40.315737Z","shell.execute_reply":"2025-05-17T11:01:44.947218Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n\n\"\"\"\nIn teoria, questo metodo permette di andare a tokenizzare e spezzare le parole per inserirle\nall'interno di BERT, o comunque per convertirle prima in uno e-branding e inserirle all'interno\ndi BERT, mantenendo perÃ² le etichette corrette. Quindi se spezzo una parola lunga, che era un \nBFOOD, ci saranno alla fine due BFOOD, in teoria.\n\"\"\"\ndef tokenize_and_align_labels(example):\n    tokenized = tokenizer(example[\"tokens\"], is_split_into_words=True, truncation=True)\n    \n    word_ids = tokenized.word_ids()\n    labels = []\n    previous_word_idx = None\n    for word_idx in word_ids:\n        if word_idx is None:\n            labels.append(-100)\n        elif word_idx != previous_word_idx:\n            labels.append(example[\"labels\"][word_idx])\n        else:\n            # Se un word viene splittato in piÃ¹ subtoken, replichiamo la label (o metti -100 se vuoi ignorare)\n            labels.append(example[\"labels\"][word_idx])\n        previous_word_idx = word_idx\n    tokenized[\"labels\"] = labels\n    return tokenized\n\ntokenized_dataset = dataset.map(tokenize_and_align_labels, batched=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T11:01:44.948612Z","iopub.execute_input":"2025-05-17T11:01:44.948834Z","iopub.status.idle":"2025-05-17T11:03:19.608005Z","shell.execute_reply.started":"2025-05-17T11:01:44.948815Z","shell.execute_reply":"2025-05-17T11:03:19.607353Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f05cf7203c242dca2e1a0a054c9b30c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"643bfc92720c4519b8af191d935dd14c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca835a1dbde3475cb1baacb2b11dda76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22e775c00f8444b1a752462691d7ff8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/80000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3633a4e7eac7454793f0e19fe578ff3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c8da68b25de48849d82bf828e6ce817"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"\ndef compute_metrics(pred):\n    predictions, labels = pred\n    predictions = predictions.argmax(axis=2)\n\n    true_labels = []\n    true_preds = []\n\n    for pred_seq, label_seq in zip(predictions, labels):\n        curr_preds = []\n        curr_labels = []\n        for p, l in zip(pred_seq, label_seq):\n            if l != -100:\n                curr_preds.append(id2tag[p])\n                curr_labels.append(id2tag[l])\n        true_preds.append(curr_preds)\n        true_labels.append(curr_labels)\n\n    return {\n        \"precision\": precision_score(true_labels, true_preds),\n        \"recall\": recall_score(true_labels, true_preds),\n        \"f1\": f1_score(true_labels, true_preds)\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T11:03:19.608975Z","iopub.execute_input":"2025-05-17T11:03:19.609201Z","iopub.status.idle":"2025-05-17T11:03:19.614724Z","shell.execute_reply.started":"2025-05-17T11:03:19.609184Z","shell.execute_reply":"2025-05-17T11:03:19.613850Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Estrai tutte le etichette\nall_labels = []\nfor example in tokenized_dataset[\"train\"]:\n    all_labels += example[\"labels\"]\n\n# Conta le etichette escludendo i -100 (token ignorati)\nlabel_counts = Counter([label for label in all_labels if label != -100])\ntotal = sum(label_counts.values())\n\n# Calcola peso inverso della frequenza (piÃ¹ rara = peso piÃ¹ alto)\nweights = [0.0] * len(tag2id)\nfor label_id, count in label_counts.items():\n    weights[label_id] = total / (len(label_counts) * count)\n\nweights = torch.tensor(weights).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T11:03:19.615572Z","iopub.execute_input":"2025-05-17T11:03:19.615830Z","iopub.status.idle":"2025-05-17T11:03:48.375776Z","shell.execute_reply.started":"2025-05-17T11:03:19.615813Z","shell.execute_reply":"2025-05-17T11:03:48.375214Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from transformers.modeling_outputs import TokenClassifierOutput\n\nclass WeightedTokenClassifier(nn.Module):\n    def __init__(self, base_model, weights):\n        super().__init__()\n        self.base_model = base_model\n        self.loss_fct = CrossEntropyLoss(weight=weights, ignore_index=-100)\n\n    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n        # Rimuove 'num_items_in_batch' se presente\n        kwargs.pop(\"num_items_in_batch\", None)\n\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n        logits = outputs.logits\n\n        loss = None\n        if labels is not None:\n            loss = self.loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states if hasattr(outputs, \"hidden_states\") else None,\n            attentions=outputs.attentions if hasattr(outputs, \"attentions\") else None,\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T11:03:48.376516Z","iopub.execute_input":"2025-05-17T11:03:48.376731Z","iopub.status.idle":"2025-05-17T11:03:48.382629Z","shell.execute_reply.started":"2025-05-17T11:03:48.376715Z","shell.execute_reply":"2025-05-17T11:03:48.381999Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"base_model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(tag2id))\nmodel = WeightedTokenClassifier(base_model, weights)\nmodel.to(device)\n\n\nargs = TrainingArguments(\n    output_dir=\"/kaggle/working/\",\n    run_name=\"bert-ner-food-v1\",  # nome run esplicito\n    do_train=True,\n    do_eval=True,\n    logging_steps=100,\n    save_steps=5000,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    report_to=\"none\",  # Disabilita logging verso wandb\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\n\"\"\"\nNel fine-tuning, serve a:uniformare la lunghezza delle sequenze (padding), \ngestire correttamente i batch, \nallineare i token con le label (soprattutto importante nel NER).\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T11:03:48.383324Z","iopub.execute_input":"2025-05-17T11:03:48.383566Z","iopub.status.idle":"2025-05-17T11:03:50.542693Z","shell.execute_reply.started":"2025-05-17T11:03:48.383542Z","shell.execute_reply":"2025-05-17T11:03:50.541882Z"}},"outputs":[{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8f6573c31d64a7d9c3cb74964aa77af"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_31/512715000.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'\\nNel fine-tuning, serve a:uniformare la lunghezza delle sequenze (padding), \\ngestire correttamente i batch, \\nallineare i token con le label (soprattutto importante nel NER).\\n'"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T11:03:50.543551Z","iopub.execute_input":"2025-05-17T11:03:50.544220Z","iopub.status.idle":"2025-05-17T13:57:47.311458Z","shell.execute_reply.started":"2025-05-17T11:03:50.544193Z","shell.execute_reply":"2025-05-17T13:57:47.310578Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15000/15000 2:53:54, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.365700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.253000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.259200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.236500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.234200</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.216100</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.210300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.216100</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.204200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.208000</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.218600</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.213800</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.197100</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.204100</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.195600</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.198400</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.190200</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.196500</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.191800</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.192100</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.187900</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.203800</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.192100</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.193300</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.194100</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.191400</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.183600</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.194100</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.199200</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.190400</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.178600</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.186800</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.177300</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.175400</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.177900</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.184700</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.174700</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.194900</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.186900</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.187500</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.185200</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.179500</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.172000</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.185300</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.175300</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.174700</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.176300</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.170800</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.164100</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.182700</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.152200</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.156300</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.156600</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.151300</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.168200</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.152100</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.159600</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.151300</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.149500</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.168900</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>0.150400</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.154100</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>0.159100</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.161400</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.157100</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.157900</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>0.159500</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>0.172500</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>0.155100</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.159200</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>0.157100</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>0.158400</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>0.165000</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>0.156400</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.155300</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>0.152500</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>0.153600</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>0.146200</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>0.138100</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.143400</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>0.153500</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>0.151300</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>0.167300</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>0.146700</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.162100</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>0.157300</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>0.148300</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>0.156100</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>0.162100</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.155600</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>0.157400</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>0.162500</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>0.151000</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>0.142500</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.148900</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>0.146900</td>\n    </tr>\n    <tr>\n      <td>9700</td>\n      <td>0.154500</td>\n    </tr>\n    <tr>\n      <td>9800</td>\n      <td>0.154800</td>\n    </tr>\n    <tr>\n      <td>9900</td>\n      <td>0.158600</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.152200</td>\n    </tr>\n    <tr>\n      <td>10100</td>\n      <td>0.128000</td>\n    </tr>\n    <tr>\n      <td>10200</td>\n      <td>0.131300</td>\n    </tr>\n    <tr>\n      <td>10300</td>\n      <td>0.126800</td>\n    </tr>\n    <tr>\n      <td>10400</td>\n      <td>0.139100</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.121600</td>\n    </tr>\n    <tr>\n      <td>10600</td>\n      <td>0.122400</td>\n    </tr>\n    <tr>\n      <td>10700</td>\n      <td>0.124000</td>\n    </tr>\n    <tr>\n      <td>10800</td>\n      <td>0.134200</td>\n    </tr>\n    <tr>\n      <td>10900</td>\n      <td>0.133900</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.135500</td>\n    </tr>\n    <tr>\n      <td>11100</td>\n      <td>0.125800</td>\n    </tr>\n    <tr>\n      <td>11200</td>\n      <td>0.133400</td>\n    </tr>\n    <tr>\n      <td>11300</td>\n      <td>0.129300</td>\n    </tr>\n    <tr>\n      <td>11400</td>\n      <td>0.137700</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.121400</td>\n    </tr>\n    <tr>\n      <td>11600</td>\n      <td>0.126200</td>\n    </tr>\n    <tr>\n      <td>11700</td>\n      <td>0.124000</td>\n    </tr>\n    <tr>\n      <td>11800</td>\n      <td>0.127900</td>\n    </tr>\n    <tr>\n      <td>11900</td>\n      <td>0.130300</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.127200</td>\n    </tr>\n    <tr>\n      <td>12100</td>\n      <td>0.115400</td>\n    </tr>\n    <tr>\n      <td>12200</td>\n      <td>0.124500</td>\n    </tr>\n    <tr>\n      <td>12300</td>\n      <td>0.137200</td>\n    </tr>\n    <tr>\n      <td>12400</td>\n      <td>0.124200</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.124100</td>\n    </tr>\n    <tr>\n      <td>12600</td>\n      <td>0.124200</td>\n    </tr>\n    <tr>\n      <td>12700</td>\n      <td>0.133300</td>\n    </tr>\n    <tr>\n      <td>12800</td>\n      <td>0.122900</td>\n    </tr>\n    <tr>\n      <td>12900</td>\n      <td>0.133700</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.130500</td>\n    </tr>\n    <tr>\n      <td>13100</td>\n      <td>0.140100</td>\n    </tr>\n    <tr>\n      <td>13200</td>\n      <td>0.123900</td>\n    </tr>\n    <tr>\n      <td>13300</td>\n      <td>0.122800</td>\n    </tr>\n    <tr>\n      <td>13400</td>\n      <td>0.131500</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.127200</td>\n    </tr>\n    <tr>\n      <td>13600</td>\n      <td>0.134100</td>\n    </tr>\n    <tr>\n      <td>13700</td>\n      <td>0.128800</td>\n    </tr>\n    <tr>\n      <td>13800</td>\n      <td>0.124900</td>\n    </tr>\n    <tr>\n      <td>13900</td>\n      <td>0.134100</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.122100</td>\n    </tr>\n    <tr>\n      <td>14100</td>\n      <td>0.126000</td>\n    </tr>\n    <tr>\n      <td>14200</td>\n      <td>0.129100</td>\n    </tr>\n    <tr>\n      <td>14300</td>\n      <td>0.130900</td>\n    </tr>\n    <tr>\n      <td>14400</td>\n      <td>0.124500</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.126800</td>\n    </tr>\n    <tr>\n      <td>14600</td>\n      <td>0.127300</td>\n    </tr>\n    <tr>\n      <td>14700</td>\n      <td>0.133200</td>\n    </tr>\n    <tr>\n      <td>14800</td>\n      <td>0.123700</td>\n    </tr>\n    <tr>\n      <td>14900</td>\n      <td>0.122600</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.132500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=15000, training_loss=0.1607112346013387, metrics={'train_runtime': 10435.7333, 'train_samples_per_second': 22.998, 'train_steps_per_second': 1.437, 'total_flos': 0.0, 'train_loss': 0.1607112346013387, 'epoch': 3.0})"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T13:57:47.312336Z","iopub.execute_input":"2025-05-17T13:57:47.312590Z","iopub.status.idle":"2025-05-17T14:01:09.321573Z","shell.execute_reply.started":"2025-05-17T13:57:47.312564Z","shell.execute_reply":"2025-05-17T14:01:09.320895Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2500/2500 03:08]\n    </div>\n    "},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.19696687161922455,\n 'eval_precision': 0.38602867455384,\n 'eval_recall': 0.9300345418971473,\n 'eval_f1': 0.5455968938422406,\n 'eval_runtime': 201.9804,\n 'eval_samples_per_second': 99.02,\n 'eval_steps_per_second': 12.377,\n 'epoch': 3.0}"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"#### Test with Lora","metadata":{}},{"cell_type":"code","source":"!pip install peft accelerate -q\n\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom transformers import AutoModelForTokenClassification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:01:09.322288Z","iopub.execute_input":"2025-05-17T14:01:09.322546Z","iopub.status.idle":"2025-05-17T14:01:12.509994Z","shell.execute_reply.started":"2025-05-17T14:01:09.322518Z","shell.execute_reply":"2025-05-17T14:01:12.509130Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"base_model = AutoModelForTokenClassification.from_pretrained(\n    \"bert-base-cased\",\n    num_labels=len(tag2id)\n)\n\npeft_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.TOKEN_CLS\n)\n\n# Applica LoRA\nmodel = get_peft_model(base_model, peft_config)\nmodel.print_trainable_parameters()  # Verifica i parametri LoRA addestrabili\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:01:12.511043Z","iopub.execute_input":"2025-05-17T14:01:12.511305Z","iopub.status.idle":"2025-05-17T14:01:12.770689Z","shell.execute_reply.started":"2025-05-17T14:01:12.511281Z","shell.execute_reply":"2025-05-17T14:01:12.770112Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 297,219 || all params: 108,019,206 || trainable%: 0.2752\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Parametri di training\nargs = TrainingArguments(\n    output_dir=\"/kaggle/working/ner-lora/\",\n    run_name=\"bert-ner-lora\",\n    do_train=True,\n    do_eval=True,\n    logging_dir=\"/kaggle/working/logs\",\n    logging_steps=100,\n    save_steps=5000,\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=8,\n    weight_decay=0.01,\n    report_to=\"none\"\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:01:12.771356Z","iopub.execute_input":"2025-05-17T14:01:12.771566Z","iopub.status.idle":"2025-05-17T14:01:12.998195Z","shell.execute_reply.started":"2025-05-17T14:01:12.771550Z","shell.execute_reply":"2025-05-17T14:01:12.997462Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/444641884.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"trainer.train()\n\n# Valutazione finale\nmetrics = trainer.evaluate()\nprint(\"Evaluation Results:\", metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:01:12.999504Z","iopub.execute_input":"2025-05-17T14:01:12.999750Z","iopub.status.idle":"2025-05-17T16:21:33.339678Z","shell.execute_reply.started":"2025-05-17T14:01:12.999733Z","shell.execute_reply":"2025-05-17T16:21:33.339093Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15000/15000 2:16:47, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.512300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.159500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.131700</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.126500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.124700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.116900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.108500</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.103600</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.102500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.099900</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.098100</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.096700</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.099600</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.096100</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.100500</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.094300</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.095500</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.088400</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.093700</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.092900</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.092300</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.092100</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.086400</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.089300</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.091500</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.089700</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.091200</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.091800</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.093100</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.091500</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.090700</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.091500</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.090400</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.087200</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.084300</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.089800</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.087100</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.086000</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.088700</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.087600</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.088600</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.085500</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.084700</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.086500</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.084400</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.085800</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.084000</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.083600</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.084700</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.085100</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.084100</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.086500</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.086900</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.082300</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.081300</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.087600</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.085000</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.082100</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.081200</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.085900</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>0.083300</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.083600</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>0.084300</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.081600</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.085200</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.084700</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>0.084200</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>0.082800</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>0.083300</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.084400</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>0.083000</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>0.087300</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>0.084500</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>0.083500</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.083100</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>0.083800</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>0.079900</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>0.079200</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>0.080900</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.079400</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>0.083500</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>0.083000</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>0.080800</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>0.079700</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.084900</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>0.082900</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>0.080100</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>0.080000</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>0.083100</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.081700</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>0.081600</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>0.084100</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>0.082000</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>0.081200</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.082600</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>0.083300</td>\n    </tr>\n    <tr>\n      <td>9700</td>\n      <td>0.083600</td>\n    </tr>\n    <tr>\n      <td>9800</td>\n      <td>0.084800</td>\n    </tr>\n    <tr>\n      <td>9900</td>\n      <td>0.082400</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.081300</td>\n    </tr>\n    <tr>\n      <td>10100</td>\n      <td>0.079400</td>\n    </tr>\n    <tr>\n      <td>10200</td>\n      <td>0.081600</td>\n    </tr>\n    <tr>\n      <td>10300</td>\n      <td>0.078000</td>\n    </tr>\n    <tr>\n      <td>10400</td>\n      <td>0.082300</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.081300</td>\n    </tr>\n    <tr>\n      <td>10600</td>\n      <td>0.080400</td>\n    </tr>\n    <tr>\n      <td>10700</td>\n      <td>0.082100</td>\n    </tr>\n    <tr>\n      <td>10800</td>\n      <td>0.084900</td>\n    </tr>\n    <tr>\n      <td>10900</td>\n      <td>0.081600</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.085200</td>\n    </tr>\n    <tr>\n      <td>11100</td>\n      <td>0.081000</td>\n    </tr>\n    <tr>\n      <td>11200</td>\n      <td>0.078700</td>\n    </tr>\n    <tr>\n      <td>11300</td>\n      <td>0.084700</td>\n    </tr>\n    <tr>\n      <td>11400</td>\n      <td>0.084900</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.080000</td>\n    </tr>\n    <tr>\n      <td>11600</td>\n      <td>0.081400</td>\n    </tr>\n    <tr>\n      <td>11700</td>\n      <td>0.080100</td>\n    </tr>\n    <tr>\n      <td>11800</td>\n      <td>0.082800</td>\n    </tr>\n    <tr>\n      <td>11900</td>\n      <td>0.077900</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.079600</td>\n    </tr>\n    <tr>\n      <td>12100</td>\n      <td>0.078700</td>\n    </tr>\n    <tr>\n      <td>12200</td>\n      <td>0.080200</td>\n    </tr>\n    <tr>\n      <td>12300</td>\n      <td>0.082900</td>\n    </tr>\n    <tr>\n      <td>12400</td>\n      <td>0.079900</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.080500</td>\n    </tr>\n    <tr>\n      <td>12600</td>\n      <td>0.078500</td>\n    </tr>\n    <tr>\n      <td>12700</td>\n      <td>0.081500</td>\n    </tr>\n    <tr>\n      <td>12800</td>\n      <td>0.081200</td>\n    </tr>\n    <tr>\n      <td>12900</td>\n      <td>0.083100</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.082500</td>\n    </tr>\n    <tr>\n      <td>13100</td>\n      <td>0.080800</td>\n    </tr>\n    <tr>\n      <td>13200</td>\n      <td>0.078100</td>\n    </tr>\n    <tr>\n      <td>13300</td>\n      <td>0.083400</td>\n    </tr>\n    <tr>\n      <td>13400</td>\n      <td>0.079400</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.082500</td>\n    </tr>\n    <tr>\n      <td>13600</td>\n      <td>0.080300</td>\n    </tr>\n    <tr>\n      <td>13700</td>\n      <td>0.079800</td>\n    </tr>\n    <tr>\n      <td>13800</td>\n      <td>0.081200</td>\n    </tr>\n    <tr>\n      <td>13900</td>\n      <td>0.080900</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.078100</td>\n    </tr>\n    <tr>\n      <td>14100</td>\n      <td>0.080900</td>\n    </tr>\n    <tr>\n      <td>14200</td>\n      <td>0.080000</td>\n    </tr>\n    <tr>\n      <td>14300</td>\n      <td>0.078600</td>\n    </tr>\n    <tr>\n      <td>14400</td>\n      <td>0.079900</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.081800</td>\n    </tr>\n    <tr>\n      <td>14600</td>\n      <td>0.078600</td>\n    </tr>\n    <tr>\n      <td>14700</td>\n      <td>0.078600</td>\n    </tr>\n    <tr>\n      <td>14800</td>\n      <td>0.083600</td>\n    </tr>\n    <tr>\n      <td>14900</td>\n      <td>0.079000</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.082600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2500/2500 03:17]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.07875818759202957, 'eval_precision': 0.5997103441504247, 'eval_recall': 0.5901350274161211, 'eval_f1': 0.5948841569572786, 'eval_runtime': 210.6964, 'eval_samples_per_second': 94.923, 'eval_steps_per_second': 11.865, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":25}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11601525,"sourceType":"datasetVersion","datasetId":7276106}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:37:54.544494Z","iopub.execute_input":"2025-05-15T08:37:54.544817Z","iopub.status.idle":"2025-05-15T08:37:57.866383Z","shell.execute_reply.started":"2025-05-15T08:37:54.544794Z","shell.execute_reply":"2025-05-15T08:37:57.865640Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:37:57.868221Z","iopub.execute_input":"2025-05-15T08:37:57.869071Z","iopub.status.idle":"2025-05-15T08:37:57.872580Z","shell.execute_reply.started":"2025-05-15T08:37:57.869047Z","shell.execute_reply":"2025-05-15T08:37:57.871987Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom datasets import Dataset\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt')  \n\n\ndf = pd.read_csv(\"/kaggle/input/recipe-sampled-0-25/sampled_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:37:57.873274Z","iopub.execute_input":"2025-05-15T08:37:57.873486Z","iopub.status.idle":"2025-05-15T08:38:05.379661Z","shell.execute_reply.started":"2025-05-15T08:37:57.873461Z","shell.execute_reply":"2025-05-15T08:38:05.378855Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"#### Preparazione del dataset","metadata":{}},{"cell_type":"code","source":"df_sample = df[[\"directions\", \"ingredients\"]].sample(n=10000, random_state=42).reset_index(drop=True)\ndf_sample.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:38:05.395128Z","iopub.execute_input":"2025-05-15T08:38:05.395395Z","iopub.status.idle":"2025-05-15T08:38:05.517440Z","shell.execute_reply.started":"2025-05-15T08:38:05.395360Z","shell.execute_reply":"2025-05-15T08:38:05.516667Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"                                          directions  \\\n0  ['Mix together the cheese, olives, onion, drie...   \n1  ['Brown meat; drain and set aside.', 'Blend ma...   \n2  ['Dissolve jello in boiling water.', 'Let cool...   \n\n                                         ingredients  \n0  [\"1 cup shredded cheddar cheese\", \"1 cup chopp...  \n1  [\"1 pie crust\", \"1/2 lb. ground beef (you can ...  \n2  [\"2 small orange jello\", \"2 c. boiling water\",...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>directions</th>\n      <th>ingredients</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>['Mix together the cheese, olives, onion, drie...</td>\n      <td>[\"1 cup shredded cheddar cheese\", \"1 cup chopp...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>['Brown meat; drain and set aside.', 'Blend ma...</td>\n      <td>[\"1 pie crust\", \"1/2 lb. ground beef (you can ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>['Dissolve jello in boiling water.', 'Let cool...</td>\n      <td>[\"2 small orange jello\", \"2 c. boiling water\",...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"import ast\n\ndf_sample[\"ingredients\"] = df_sample[\"ingredients\"].apply(ast.literal_eval)\ndf_sample[\"directions\"] = df_sample[\"directions\"].apply(ast.literal_eval)\n\nprint(type(df_sample.loc[0, \"ingredients\"]))  # deve essere <class 'list'>\nprint(df_sample.loc[0, \"ingredients\"])        # stampa la lista vera\n\nprint(type(df_sample.loc[0, \"directions\"]))  # deve essere <class 'list'>\nprint(df_sample.loc[0, \"directions\"])        # stampa la lista vera","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:38:05.518573Z","iopub.execute_input":"2025-05-15T08:38:05.518931Z","iopub.status.idle":"2025-05-15T08:38:05.915800Z","shell.execute_reply.started":"2025-05-15T08:38:05.518912Z","shell.execute_reply":"2025-05-15T08:38:05.915070Z"}},"outputs":[{"name":"stdout","text":"<class 'list'>\n['1 cup shredded cheddar cheese', '1 cup chopped pimento stuffed olive', '1 tablespoon minced onion', '1 cup dried beef, chopped', '3/4 - 1 cup mayonnaise', '1 loaf sliced rye cocktail bread']\n<class 'list'>\n['Mix together the cheese, olives, onion, dried beef and mayo.', 'Spread on slices of rye cocktail bread. place the slices on a cookie sheet and broil until bubbly.']\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"df_sample[\"text\"] = df_sample[\"directions\"].apply(lambda steps: \" \".join(steps))\nprint(df_sample.loc[0, \"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:38:05.916469Z","iopub.execute_input":"2025-05-15T08:38:05.916691Z","iopub.status.idle":"2025-05-15T08:38:05.929678Z","shell.execute_reply.started":"2025-05-15T08:38:05.916674Z","shell.execute_reply":"2025-05-15T08:38:05.928800Z"}},"outputs":[{"name":"stdout","text":"Mix together the cheese, olives, onion, dried beef and mayo. Spread on slices of rye cocktail bread. place the slices on a cookie sheet and broil until bubbly.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"Queste regole regex non sono complete, non coprono tutti i casi. Bisognerebbe aggiunge man mano, ma √® una operazione complicata","metadata":{}},{"cell_type":"code","source":"import re\n\ndef clean_ingredient(ingredient):\n    # Remove fractions and numbers (e.g., \"1\", \"1/2\", \"2.5\")\n    ingredient = re.sub(r'\\b\\d+([\\/\\.]\\d+)?\\b', '', ingredient)\n\n    # Common measurement units to remove\n    units = [\n        \"teaspoons?\", \"tsp\", \"tablespoons?\", \"tbsp\", \"cups?\", \"ounces?\", \"oz\",\n        \"pounds?\", \"lb\", \"grams?\", \"g\", \"kilograms?\", \"kg\", \"milliliters?\", \"ml\",\n        \"liters?\", \"l\", \"pinch\", \"clove\", \"cloves\", \"slices?\", \"dash\", \"cans?\", \n        \"packages?\", \"bunch\", \"stalks?\", \"heads?\", \"pieces?\", \"sticks?\", \"inches?\"\n    ]\n    units_pattern = r'\\b(?:' + '|'.join(units) + r')\\b'\n    ingredient = re.sub(units_pattern, '', ingredient, flags=re.IGNORECASE)\n\n    ingredient = re.sub(r'\\b(c\\.|c)\\b\\.?', '', ingredient, flags=re.IGNORECASE)\n\n    ingredient = re.sub(r'\\(\\s*\\.\\s*\\)', '', ingredient)\n    ingredient = re.sub(r'\\([^)]*\\)', '', ingredient)\n\n    ingredient = re.sub(r'\\bof\\b', '', ingredient, flags=re.IGNORECASE)\n    ingredient = re.sub(r'^\\s*\\.\\s*', '', ingredient)       # punto iniziale con spazio\n    ingredient = re.sub(r'\\.\\s*', ' ', ingredient)          # ogni \". \" ovunque\n    ingredient = re.sub(r',.*', '', ingredient)             # rimuove note dopo virgola\n\n    ingredient = re.sub(r'\\b(to |pt |pkg |qt )\\.?\\b', '', ingredient, flags=re.IGNORECASE)\n    ingredient = re.sub(r'^to\\s+', '', ingredient, flags=re.IGNORECASE)\n\n    # Remove extra spaces\n    ingredient = re.sub(r'\\s+', ' ', ingredient).strip()\n\n    return ingredient\n\n\n# Applica a tutta la colonna ingredients\ndf_sample[\"clean_ingredients\"] = df_sample[\"ingredients\"].apply(lambda lst: [clean_ingredient(i) for i in lst])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:38:05.930469Z","iopub.execute_input":"2025-05-15T08:38:05.930681Z","iopub.status.idle":"2025-05-15T08:38:07.255248Z","shell.execute_reply.started":"2025-05-15T08:38:05.930637Z","shell.execute_reply":"2025-05-15T08:38:07.254693Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"print(df_sample.loc[:10, \"clean_ingredients\"]) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:38:07.255955Z","iopub.execute_input":"2025-05-15T08:38:07.256162Z","iopub.status.idle":"2025-05-15T08:38:07.262274Z","shell.execute_reply.started":"2025-05-15T08:38:07.256146Z","shell.execute_reply":"2025-05-15T08:38:07.261367Z"}},"outputs":[{"name":"stdout","text":"0     [shredded cheddar cheese, chopped pimento stuf...\n1     [pie crust, ground beef, mayonnaise, milk, egg...\n2     [small orange jello, boiling water, small crus...\n3     [square graham crackers, reduced calorie marga...\n4     [cream cheese, sm jar Old English cheese, Lipt...\n5     [MIRACLE WHIP Dressing, BREAKSTONE'S or KNUDSE...\n6     [FOR THE FILLING:, Fresh Strawberries, - Fresh...\n7     [doz mangos, cabbage, celery, brown sugar, sal...\n8     [chopped green peppers, chopped red peppers, c...\n9     [fryer, uncooked rice, cream chicken soup, dry...\n10    [yeast, bread flour, salt, sugar, olive oil, w...\nName: clean_ingredients, dtype: object\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"Assegno le label \"0\" \"I-food\" \"B-food\"","metadata":{}},{"cell_type":"code","source":"\ndef check(labels):\n      # üîç Verifica coerenza: nessun I-FOOD senza un B-FOOD prima\n    for i, label in enumerate(labels):\n        if label == 'I-FOOD':\n            if i == 0 or labels[i - 1] not in ['B-FOOD', 'I-FOOD']:\n                raise ValueError(f\"Incoerenza IOB: I-FOOD alla posizione {i} senza B-FOOD precedente.\")\n\n\n\ndef iob_tag_tokens(text, ingredient_list):\n    tokens = word_tokenize(text)\n    labels = ['O'] * len(tokens)\n    \n    for ingredient in ingredient_list:\n        ingredient_tokens = word_tokenize(ingredient)\n        ingredient_len = len(ingredient_tokens)\n\n        if ingredient_len == 0:\n            continue  # ignora ingredienti vuoti\n\n        for i in range(len(tokens) - ingredient_len + 1):\n            window = tokens[i:i + ingredient_len]\n            if [t.lower() for t in window] == [t.lower() for t in ingredient_tokens]:\n                labels[i] = 'B-FOOD'\n                for j in range(1, ingredient_len):\n                    if i + j < len(labels):\n                        labels[i + j] = 'I-FOOD'\n                break  # evita doppi match dello stesso ingrediente\n\n\n    check(labels)\n    \n    return tokens, labels\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:38:07.263168Z","iopub.execute_input":"2025-05-15T08:38:07.264036Z","iopub.status.idle":"2025-05-15T08:38:07.278833Z","shell.execute_reply.started":"2025-05-15T08:38:07.264011Z","shell.execute_reply":"2025-05-15T08:38:07.278272Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"df_sample[\"ner_tokens_labels\"] = df_sample.apply(\n    lambda row: iob_tag_tokens(row[\"text\"], row[\"clean_ingredients\"]), axis=1\n)\n\"\"\"\nESEMPIO UTILIZZO:\ntext = \"Aggiungi una cipolla tritata e soffriggi in olio.\"\nclean_ingredients = [\"cipolla\", \"olio\"]\n\ntokens = [\"Aggiungi\", \"una\", \"cipolla\", \"tritata\", \"e\", \"soffriggi\", \"in\", \"olio\", \".\"]\nlabels = [\"O\",        \"O\",   \"B-FOOD\", \"I-FOOD\",  \"O\", \"O\",         \"O\", \"B-FOOD\", \"O\"]\n\nRISULTATO FINALE:\n(\"Aggiungi\", ..., \"olio\", \".\"), [\"O\", ..., \"B-FOOD\", \"O\"]\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:38:07.281236Z","iopub.execute_input":"2025-05-15T08:38:07.281427Z","iopub.status.idle":"2025-05-15T08:38:23.737893Z","shell.execute_reply.started":"2025-05-15T08:38:07.281413Z","shell.execute_reply":"2025-05-15T08:38:23.737311Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"'\\nESEMPIO UTILIZZO:\\ntext = \"Aggiungi una cipolla tritata e soffriggi in olio.\"\\nclean_ingredients = [\"cipolla\", \"olio\"]\\n\\ntokens = [\"Aggiungi\", \"una\", \"cipolla\", \"tritata\", \"e\", \"soffriggi\", \"in\", \"olio\", \".\"]\\nlabels = [\"O\",        \"O\",   \"B-FOOD\", \"I-FOOD\",  \"O\", \"O\",         \"O\", \"B-FOOD\", \"O\"]\\n\\nRISULTATO FINALE:\\n(\"Aggiungi\", ..., \"olio\", \".\"), [\"O\", ..., \"B-FOOD\", \"O\"]\\n'"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"tokens, labels = df_sample.loc[0, \"ner_tokens_labels\"]\nfor t, l in zip(tokens, labels):\n    print(f\"{t:15} ‚Üí {l}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:38:23.738589Z","iopub.execute_input":"2025-05-15T08:38:23.738908Z","iopub.status.idle":"2025-05-15T08:38:23.743918Z","shell.execute_reply.started":"2025-05-15T08:38:23.738889Z","shell.execute_reply":"2025-05-15T08:38:23.743051Z"}},"outputs":[{"name":"stdout","text":"Mix             ‚Üí O\ntogether        ‚Üí O\nthe             ‚Üí O\ncheese          ‚Üí O\n,               ‚Üí O\nolives          ‚Üí O\n,               ‚Üí O\nonion           ‚Üí O\n,               ‚Üí O\ndried           ‚Üí B-FOOD\nbeef            ‚Üí I-FOOD\nand             ‚Üí O\nmayo            ‚Üí O\n.               ‚Üí O\nSpread          ‚Üí O\non              ‚Üí O\nslices          ‚Üí O\nof              ‚Üí O\nrye             ‚Üí O\ncocktail        ‚Üí O\nbread           ‚Üí O\n.               ‚Üí O\nplace           ‚Üí O\nthe             ‚Üí O\nslices          ‚Üí O\non              ‚Üí O\na               ‚Üí O\ncookie          ‚Üí O\nsheet           ‚Üí O\nand             ‚Üí O\nbroil           ‚Üí O\nuntil           ‚Üí O\nbubbly          ‚Üí O\n.               ‚Üí O\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"#### Preparazione dell'addestramento","metadata":{}},{"cell_type":"code","source":"!pip install seqeval\n\nfrom datasets import Dataset, ClassLabel\nfrom transformers import AutoTokenizer\nfrom seqeval.metrics import precision_score, recall_score, f1_score\nfrom transformers import DataCollatorForTokenClassification\nfrom transformers import EarlyStoppingCallback\nfrom transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\nfrom seqeval.metrics import precision_score, recall_score, f1_score\nfrom torch.nn import CrossEntropyLoss\nfrom collections import Counter\nimport torch.nn as nn\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:38:23.744567Z","iopub.execute_input":"2025-05-15T08:38:23.744814Z","iopub.status.idle":"2025-05-15T08:38:26.851306Z","shell.execute_reply.started":"2025-05-15T08:38:23.744791Z","shell.execute_reply":"2025-05-15T08:38:26.850531Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"\"\"\"\n([\"Aggiungi\", \"una\", \"cipolla\", \"tritata\", \"finemente\", ...],\n [\"O\",       \"O\",   \"B-FOOD\", \"I-FOOD\", \"O\", ...])\nDIVENTA:\n{\n    \"tokens\": [\"Aggiungi\", \"una\", \"cipolla\", \"tritata\", \"finemente\", ...],\n    \"ner_tags\": [\"O\", \"O\", \"B-FOOD\", \"I-FOOD\", \"O\", ...]\n}\n\"\"\"\n\nhf_data = [\n    {\n        \"tokens\": tokens,\n        \"ner_tags\": labels\n    }\n    for tokens, labels in df_sample[\"ner_tokens_labels\"]\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:38:26.852379Z","iopub.execute_input":"2025-05-15T08:38:26.852679Z","iopub.status.idle":"2025-05-15T08:38:26.948060Z","shell.execute_reply.started":"2025-05-15T08:38:26.852626Z","shell.execute_reply":"2025-05-15T08:38:26.947231Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"unique_tags = set(tag for row in hf_data for tag in row[\"ner_tags\"])\nunique_tags = sorted(unique_tags)\n\ntag2id = {tag: i for i, tag in enumerate(unique_tags)}\nid2tag = {i: tag for tag, i in tag2id.items()}\n\"\"\"\ntag2id = {\"B-FOOD\": 0, \"I-FOOD\": 1, \"O\": 2}\nid2tag = {0: \"B-FOOD\", 1: \"I-FOOD\", 2: \"O\"}\n\"\"\"\n\n#Sostituisce \"ner_tags\" con una nuova chiave \"labels\" contenente gli ID\nfor row in hf_data:\n    row[\"labels\"] = [tag2id[tag] for tag in row[\"ner_tags\"]]\n    del row[\"ner_tags\"]  \n\n#Conversione in un effetivo dataset\ndataset = Dataset.from_list(hf_data)\ndataset = dataset.train_test_split(test_size=0.1, seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:38:26.950048Z","iopub.execute_input":"2025-05-15T08:38:26.950280Z","iopub.status.idle":"2025-05-15T08:38:27.420167Z","shell.execute_reply.started":"2025-05-15T08:38:26.950264Z","shell.execute_reply":"2025-05-15T08:38:27.419690Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n\n\"\"\"\nIn teoria, questo metodo permette di andare a tokenizzare e spezzare le parole per inserirle\nall'interno di BERT, o comunque per convertirle prima in uno e-branding e inserirle all'interno\ndi BERT, mantenendo per√≤ le etichette corrette. Quindi se spezzo una parola lunga, che era un \nBFOOD, ci saranno alla fine due BFOOD, in teoria.\n\"\"\"\ndef tokenize_and_align_labels(example):\n    tokenized = tokenizer(example[\"tokens\"], is_split_into_words=True, truncation=True)\n    \n    word_ids = tokenized.word_ids()\n    labels = []\n    previous_word_idx = None\n    for word_idx in word_ids:\n        if word_idx is None:\n            labels.append(-100)\n        elif word_idx != previous_word_idx:\n            labels.append(example[\"labels\"][word_idx])\n        else:\n            # Se un word viene splittato in pi√π subtoken, replichiamo la label (o metti -100 se vuoi ignorare)\n            labels.append(example[\"labels\"][word_idx])\n        previous_word_idx = word_idx\n    tokenized[\"labels\"] = labels\n    return tokenized\n\ntokenized_dataset = dataset.map(tokenize_and_align_labels, batched=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:38:27.421048Z","iopub.execute_input":"2025-05-15T08:38:27.421701Z","iopub.status.idle":"2025-05-15T08:38:37.668836Z","shell.execute_reply.started":"2025-05-15T08:38:27.421674Z","shell.execute_reply":"2025-05-15T08:38:37.668069Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d7d60fd419d47eaaa0c69c2a183e50f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44a7560fb5ee4a83a96884ccf0a026c1"}},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"\ndef compute_metrics(pred):\n    predictions, labels = pred\n    predictions = predictions.argmax(axis=2)\n\n    true_labels = []\n    true_preds = []\n\n    for pred_seq, label_seq in zip(predictions, labels):\n        curr_preds = []\n        curr_labels = []\n        for p, l in zip(pred_seq, label_seq):\n            if l != -100:\n                curr_preds.append(id2tag[p])\n                curr_labels.append(id2tag[l])\n        true_preds.append(curr_preds)\n        true_labels.append(curr_labels)\n\n    return {\n        \"precision\": precision_score(true_labels, true_preds),\n        \"recall\": recall_score(true_labels, true_preds),\n        \"f1\": f1_score(true_labels, true_preds)\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:38:37.669843Z","iopub.execute_input":"2025-05-15T08:38:37.670177Z","iopub.status.idle":"2025-05-15T08:38:37.675423Z","shell.execute_reply.started":"2025-05-15T08:38:37.670153Z","shell.execute_reply":"2025-05-15T08:38:37.674882Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Estrai tutte le etichette\nall_labels = []\nfor example in tokenized_dataset[\"train\"]:\n    all_labels += example[\"labels\"]\n\n# Conta le etichette escludendo i -100 (token ignorati)\nlabel_counts = Counter([label for label in all_labels if label != -100])\ntotal = sum(label_counts.values())\n\n# Calcola peso inverso della frequenza (pi√π rara = peso pi√π alto)\nweights = [0.0] * len(tag2id)\nfor label_id, count in label_counts.items():\n    weights[label_id] = total / (len(label_counts) * count)\n\nweights = torch.tensor(weights).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:38:37.676182Z","iopub.execute_input":"2025-05-15T08:38:37.676475Z","iopub.status.idle":"2025-05-15T08:38:40.744318Z","shell.execute_reply.started":"2025-05-15T08:38:37.676451Z","shell.execute_reply":"2025-05-15T08:38:40.743767Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"from transformers.modeling_outputs import TokenClassifierOutput\n\nclass WeightedTokenClassifier(nn.Module):\n    def __init__(self, base_model, weights):\n        super().__init__()\n        self.base_model = base_model\n        self.loss_fct = CrossEntropyLoss(weight=weights, ignore_index=-100)\n\n    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n        # Rimuove 'num_items_in_batch' se presente\n        kwargs.pop(\"num_items_in_batch\", None)\n\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n        logits = outputs.logits\n\n        loss = None\n        if labels is not None:\n            loss = self.loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states if hasattr(outputs, \"hidden_states\") else None,\n            attentions=outputs.attentions if hasattr(outputs, \"attentions\") else None,\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:41:23.299738Z","iopub.execute_input":"2025-05-15T08:41:23.300356Z","iopub.status.idle":"2025-05-15T08:41:23.306866Z","shell.execute_reply.started":"2025-05-15T08:41:23.300334Z","shell.execute_reply":"2025-05-15T08:41:23.306061Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"base_model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(tag2id))\nmodel = WeightedTokenClassifier(base_model, weights)\nmodel.to(device)\n\n\nargs = TrainingArguments(\n    output_dir=\"/kaggle/working/\",\n    run_name=\"bert-ner-food-v1\",  # nome run esplicito\n    do_train=True,\n    do_eval=True,\n    logging_steps=100,\n    save_steps=500,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=8,\n    num_train_epochs=6,\n    weight_decay=0.01,\n    report_to=\"none\",  # Disabilita logging verso wandb\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\n\"\"\"\nNel fine-tuning, serve a:uniformare la lunghezza delle sequenze (padding), \ngestire correttamente i batch, \nallineare i token con le label (soprattutto importante nel NER).\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:41:23.307970Z","iopub.execute_input":"2025-05-15T08:41:23.308191Z","iopub.status.idle":"2025-05-15T08:41:23.706226Z","shell.execute_reply.started":"2025-05-15T08:41:23.308168Z","shell.execute_reply":"2025-05-15T08:41:23.705335Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_31/102190035.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"'\\nNel fine-tuning, serve a:uniformare la lunghezza delle sequenze (padding), \\ngestire correttamente i batch, \\nallineare i token con le label (soprattutto importante nel NER).\\n'"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:41:23.707848Z","iopub.execute_input":"2025-05-15T08:41:23.708070Z","iopub.status.idle":"2025-05-15T09:20:08.392833Z","shell.execute_reply.started":"2025-05-15T08:41:23.708054Z","shell.execute_reply":"2025-05-15T09:20:08.392216Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3378' max='3378' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3378/3378 38:42, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.391400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.238100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.239500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.232300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.229600</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.204800</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.193600</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.176400</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.183500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.187700</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.176000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.148300</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.150100</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.147200</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.144200</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.154700</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.136900</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.119500</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.118400</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.118400</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.118000</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.122000</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.107500</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.097200</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.097100</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.094700</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.098500</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.102600</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.085000</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.091100</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.083300</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.085800</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.086100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3378, training_loss=0.14862379722866242, metrics={'train_runtime': 2324.156, 'train_samples_per_second': 23.234, 'train_steps_per_second': 1.453, 'total_flos': 0.0, 'train_loss': 0.14862379722866242, 'epoch': 6.0})"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T09:20:08.393546Z","iopub.execute_input":"2025-05-15T09:20:08.393784Z","iopub.status.idle":"2025-05-15T09:20:18.739404Z","shell.execute_reply.started":"2025-05-15T09:20:08.393767Z","shell.execute_reply":"2025-05-15T09:20:18.738711Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 00:09]\n    </div>\n    "},"metadata":{}},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.4377688467502594,\n 'eval_precision': 0.3938333697791384,\n 'eval_recall': 0.8878481636677348,\n 'eval_f1': 0.5456335681284556,\n 'eval_runtime': 10.3365,\n 'eval_samples_per_second': 96.744,\n 'eval_steps_per_second': 12.093,\n 'epoch': 6.0}"},"metadata":{}}],"execution_count":49}]}
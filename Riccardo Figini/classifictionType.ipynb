{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2687c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch scikit-learn pandas -q\n",
    "\n",
    "# Import librerie\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.optim as optim\n",
    "\n",
    "# Verifica GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il dataset\n",
    "df = pd.read_csv(\"/kaggle/input/typeofplate/classified_recipes.csv\")\n",
    "valid_categories = ['Antipasto', 'Primo', 'Secondo', 'Dessert']\n",
    "df = df[df['Category'].isin(valid_categories)].copy()\n",
    "\n",
    "# Combina il testo\n",
    "df['text'] = df['title'].fillna('') + ' ' + df['ingredients'].fillna('') + ' ' + df['directions'].fillna('')\n",
    "\n",
    "# Encoding etichette\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['Category'])\n",
    "\n",
    "# Tokenizza testi\n",
    "df['tokens'] = df['text'].apply(lambda x: simple_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d6d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestra un modello Word2Vec sui token del dataset e calcola i vettori medi delle frasi\n",
    "\n",
    "# Addestra Word2Vec (o carica modello pre-addestrato)\n",
    "sentences = df['tokens'].tolist()\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4, epochs=10)\n",
    "\n",
    "# Funzione per vettore medio della frase\n",
    "def get_sentence_vector(tokens, model, vector_size):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "vector_size = 100\n",
    "df['vector'] = df['tokens'].apply(lambda tokens: get_sentence_vector(tokens, w2v_model, vector_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7dcd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['vector'].tolist(), df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Dataset PyTorch\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(np.array(X), dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "train_dataset = EmbeddingDataset(X_train, y_train)\n",
    "test_dataset = EmbeddingDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efea0e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modello lineare\n",
    "class SimpleLinear(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleLinear(input_dim=vector_size, num_classes=len(le.classes_)).to(device)\n",
    "\n",
    "# Training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Valutazione\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=le.classes_))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
